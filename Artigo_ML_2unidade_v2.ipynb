{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yt_dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TelpVnVcZbg",
        "outputId": "34d96fb0-f14d-4c63-fdb5-b814435f9ccd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2025.10.14-py3-none-any.whl.metadata (175 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.9/175.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.10.14-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp\n",
            "Successfully installed yt_dlp-2025.10.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5CfscjXKvl8l"
      },
      "outputs": [],
      "source": [
        "# --------------------------\n",
        "# 1 - installs, imports e diret√≥rios (ATUALIZADO)\n",
        "# --------------------------\n",
        "# se rodando em notebook: descomente a linha abaixo para garantir libs\n",
        "# !pip install numpy pandas matplotlib seaborn tqdm pillow librosa audioread yt-dlp ffmpeg-python scikit-learn xgboost tensorflow\n",
        "\n",
        "import os, re, shutil, subprocess, math, random, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa, librosa.display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K, callbacks, optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "\n",
        "# Fixar seeds\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --------------------------\n",
        "# Diret√≥rios principais (ajuste conforme seu Drive)\n",
        "# --------------------------\n",
        "\n",
        "ROOT_DIR = \"/content/drive/MyDrive/M.L_2UNIDADE\"   # caminho principal do projeto da 2¬™ unidade\n",
        "\n",
        "# Estrutura de pastas\n",
        "CSV_DIR     = os.path.join(ROOT_DIR, \"csv_files\")              # arquivos CSV do dataset\n",
        "COOKIES_DIR = os.path.join(ROOT_DIR, \"cookies\")                # cookies yt-dlp (opcional)\n",
        "WAV_DIR     = os.path.join(ROOT_DIR, \"wav_files\")              # cortes de √°udio WAV (10s)\n",
        "IMG_DIR     = os.path.join(ROOT_DIR, \"data-files\")             # espectrogramas originais\n",
        "AUG_DIR     = os.path.join(ROOT_DIR, \"data-files-augmented\")   # espectrogramas com augmentations\n",
        "MODEL_DIR   = os.path.join(ROOT_DIR, \"models_unit2\")           # modelos treinados (VAE, CNNs)\n",
        "RESULTS_DIR = os.path.join(ROOT_DIR, \"results_unit2\")          # m√©tricas, curvas ROC, matrizes de confus√£o\n",
        "\n",
        "# Criar diret√≥rios caso n√£o existam\n",
        "for d in [CSV_DIR, COOKIES_DIR, WAV_DIR, IMG_DIR, AUG_DIR, MODEL_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Caminhos espec√≠ficos\n",
        "CSV_PATH     = os.path.join(CSV_DIR, \"unbalanced_train_segments.csv\")  # caminho do CSV principal\n",
        "COOKIES_PATH = os.path.join(COOKIES_DIR, \"www.youtube.com_cookies.txt\")                 # cookies yt-dlp (se necess√°rio)\n",
        "\n",
        "# --------------------------\n",
        "# Par√¢metros principais\n",
        "# --------------------------\n",
        "TARGET_PER_CLASS = 250    # n√∫mero de m√∫sicas por g√™nero\n",
        "SR = 22050                # sample rate\n",
        "CLIP_DURATION = 10        # dura√ß√£o dos clipes em segundos\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "VAE_EPOCHS = 2\n",
        "CLASS_EPOCHS = 2\n",
        "LATENT_DIM = 64\n",
        "NOISE_FACTOR = 0.08\n",
        "SPEC_AUG_TIME_MASKS = 2\n",
        "SPEC_AUG_FREQ_MASKS = 2\n",
        "\n",
        "RANDOM_STATE = SEED\n",
        "\n",
        "# --------------------------\n",
        "# G√™neros alvo (iguais ao c√≥digo original; pode ajustar se quiser)\n",
        "# --------------------------\n",
        "genre_dict = {\n",
        "    '/m/064t9': 'Pop_music',\n",
        "    '/m/0glt670': 'Hip_hop_music',\n",
        "    '/m/06by7': 'Rock_music',\n",
        "    '/m/06j6l': 'Rhythm_blues',\n",
        "    '/m/06cqb': 'Reggae',\n",
        "    '/m/0y4f8': 'Vocal',\n",
        "    '/m/07gxw': 'Techno',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Helpers utilit√°rios\n",
        "# --------------------------\n",
        "def ensure_empty_dir(d):\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def plot_and_save_fig(fig, filename):\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(filename, dpi=200)\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "REzTtoBDyTop"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 2 - carregamento do dataset (leitura do CSV e filtragem por g√™neros)\n",
        "# --------------------------\n",
        "print(\"Etapa 2: lendo CSV e preparando candidatos...\")\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"CSV n√£o encontrado em {CSV_PATH} - verifique caminho\")\n",
        "\n",
        "data = []\n",
        "with open(CSV_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "        elements = re.sub(r'[\"\\n]', \"\", line).split(\",\")\n",
        "        if len(elements) >= 4:\n",
        "            url = elements[0]; start = elements[1]; end = elements[2]; labels = elements[3:]\n",
        "            for label in labels:\n",
        "                if label in genre_dict:\n",
        "                    data.append([url, start, end, genre_dict[label]])\n",
        "df = pd.DataFrame(data, columns=[\"url\", \"start_time\", \"end_time\", \"class_label\"])\n",
        "df.to_csv(os.path.join(ROOT_DIR, \"df_candidates_unit2.csv\"), index=False)\n",
        "print(\"Total candidatos lidos:\", len(df))\n",
        "print(df[\"class_label\"].value_counts())"
      ],
      "metadata": {
        "id": "MN4zR-23yXin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9574c3-781a-421c-c19b-3dcd7c31a0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etapa 2: lendo CSV e preparando candidatos...\n",
            "Total candidatos lidos: 51710\n",
            "class_label\n",
            "Techno           16811\n",
            "Pop_music         8407\n",
            "Rock_music        8198\n",
            "Hip_hop_music     7370\n",
            "Rhythm_blues      4755\n",
            "Vocal             3241\n",
            "Reggae            2928\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 3 - corte dos v√≠deos (download e cut) ‚Äî com retomada ordenada\n",
        "# --------------------------\n",
        "COOKIES_PATH = os.path.join(COOKIES_DIR, \"www.youtube.com_cookies.txt\")\n",
        "print(\"\\nEtapa 3: Download e corte dos v√≠deos (com retomada ordenada e verifica√ß√£o incremental).\")\n",
        "\n",
        "import yt_dlp\n",
        "import pandas as pd\n",
        "import os, subprocess\n",
        "\n",
        "# üî∏ N√ÉO limpar o diret√≥rio\n",
        "os.makedirs(WAV_DIR, exist_ok=True)\n",
        "\n",
        "# üî∏ Verificar progresso anterior\n",
        "progress_file = os.path.join(ROOT_DIR, \"df_success_unit2.csv\")\n",
        "if os.path.exists(progress_file):\n",
        "    df_success = pd.read_csv(progress_file)\n",
        "    downloaded_ids = set(df_success[\"url\"].astype(str))\n",
        "    print(f\"‚úÖ Retomando: {len(downloaded_ids)} v√≠deos j√° baixados com sucesso.\")\n",
        "else:\n",
        "    df_success = pd.DataFrame(columns=[\"url\", \"start_time\", \"end_time\", \"class_label\"])\n",
        "    downloaded_ids = set()\n",
        "\n",
        "# üî∏ Contar quantos arquivos WAV j√° existem por g√™nero\n",
        "existing_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "existing_counts = {}\n",
        "for f in existing_files:\n",
        "    try:\n",
        "        genre_name = f.split(\"_\", 1)[1][:-4]\n",
        "        existing_counts[genre_name] = existing_counts.get(genre_name, 0) + 1\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "counts = {label: existing_counts.get(label, 0) for label in genre_dict.values()}\n",
        "print(\"üìä Arquivos j√° existentes por g√™nero:\", counts)\n",
        "\n",
        "# üî∏ Determinar o √∫ltimo v√≠deo baixado com sucesso (para cada g√™nero)\n",
        "last_positions = {}\n",
        "if not df_success.empty:\n",
        "    for genre in df_success[\"class_label\"].unique():\n",
        "        last_url = df_success[df_success[\"class_label\"] == genre][\"url\"].iloc[-1]\n",
        "        try:\n",
        "            pos = df[df[\"url\"] == last_url].index[-1]\n",
        "            last_positions[genre] = pos\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "# üî∏ Fun√ß√£o auxiliar: verificar se j√° existe WAV\n",
        "def wav_exists(video_id, label):\n",
        "    pattern = f\"{video_id}_{label}.wav\"\n",
        "    return any(pattern in f for f in existing_files)\n",
        "\n",
        "# üî∏ Baixar somente os g√™neros incompletos, continuando do √∫ltimo baixado\n",
        "downloaded_rows = []\n",
        "for label, group in df.groupby(\"class_label\"):\n",
        "    current_count = counts.get(label, 0)\n",
        "    if current_count >= TARGET_PER_CLASS:\n",
        "        print(f\"‚úÖ {label} j√° completo ({current_count}/{TARGET_PER_CLASS}) - pulando.\")\n",
        "        continue\n",
        "\n",
        "    # se houver posi√ß√£o salva, come√ßa dali\n",
        "    start_index = last_positions.get(label, group.index[0])\n",
        "    group = group.loc[group.index >= start_index]\n",
        "\n",
        "    needed = TARGET_PER_CLASS - current_count\n",
        "    print(f\"\\nüîπ {label}: precisa baixar {needed} arquivos (continuando ap√≥s √≠ndice {start_index}).\")\n",
        "\n",
        "    for idx, row in group.iterrows():\n",
        "        vid = str(row[\"url\"])\n",
        "        start = float(row[\"start_time\"])\n",
        "        url = f\"https://www.youtube.com/watch?v={vid}\"\n",
        "        final_file = os.path.join(WAV_DIR, f\"{vid}_{label}.wav\")\n",
        "\n",
        "        # j√° baixado? pula\n",
        "        if vid in downloaded_ids or wav_exists(vid, label):\n",
        "            continue\n",
        "        if counts[label] >= TARGET_PER_CLASS:\n",
        "            print(f\"üéØ {label} atingiu {TARGET_PER_CLASS}, parando.\")\n",
        "            break\n",
        "\n",
        "        temp_out = os.path.join(WAV_DIR, f\"temp_{vid}.%(ext)s\")\n",
        "        try:\n",
        "            ydl_opts = {\n",
        "                \"format\": \"bestaudio/best\",\n",
        "                \"outtmpl\": temp_out,\n",
        "                \"quiet\": True,\n",
        "                \"noplaylist\": True\n",
        "            }\n",
        "            if os.path.exists(COOKIES_PATH):\n",
        "                ydl_opts[\"cookiefile\"] = COOKIES_PATH\n",
        "\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([url])\n",
        "\n",
        "            temp_files = [f for f in os.listdir(WAV_DIR) if f.startswith(f\"temp_{vid}\")]\n",
        "            if not temp_files:\n",
        "                print(f\"‚ö†Ô∏è Nenhum arquivo tempor√°rio gerado para {vid}\")\n",
        "                continue\n",
        "\n",
        "            temp_file = os.path.join(WAV_DIR, temp_files[0])\n",
        "            subprocess.run([\n",
        "                \"ffmpeg\", \"-y\", \"-ss\", str(start), \"-t\", str(CLIP_DURATION),\n",
        "                \"-i\", temp_file, \"-acodec\", \"pcm_s16le\", \"-ar\", str(SR), \"-ac\", \"1\", final_file\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            os.remove(temp_file)\n",
        "            if os.path.exists(final_file):\n",
        "                counts[label] += 1\n",
        "                downloaded_rows.append(row)\n",
        "                downloaded_ids.add(vid)\n",
        "\n",
        "                # salvar progresso incremental\n",
        "                pd.concat([df_success, pd.DataFrame([row])]).to_csv(progress_file, index=False)\n",
        "\n",
        "                if counts[label] % 10 == 0:\n",
        "                    print(f\"üéµ {counts[label]} baixados para {label}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è WAV n√£o gerado para {vid}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao baixar {vid}: {e}\")\n",
        "            continue\n",
        "\n",
        "# üî∏ Salvar resumo\n",
        "summary = pd.DataFrame(list(counts.items()), columns=[\"genre\", \"count\"])\n",
        "summary.to_csv(os.path.join(ROOT_DIR, \"dataset_summary_unit2.csv\"), index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Download finalizado.\")\n",
        "for g, c in counts.items():\n",
        "    print(f\" - {g}: {c}/{TARGET_PER_CLASS}\")\n",
        "print(\"Progresso salvo em dataset_summary_unit2.csv e df_success_unit2.csv\")\n"
      ],
      "metadata": {
        "id": "VJ2sofWSyawI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153c9427-7445-4459-c59e-9f6a1dcceeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etapa 3: Download e corte dos v√≠deos (com retomada ordenada e verifica√ß√£o incremental).\n",
            "‚úÖ Retomando: 11 v√≠deos j√° baixados com sucesso.\n",
            "üìä Arquivos j√° existentes por g√™nero: {'Pop_music': 250, 'Hip_hop_music': 250, 'Rock_music': 249, 'Rhythm_blues': 250, 'Reggae': 250, 'Vocal': 234, 'Techno': 250}\n",
            "‚úÖ Hip_hop_music j√° completo (250/250) - pulando.\n",
            "‚úÖ Pop_music j√° completo (250/250) - pulando.\n",
            "‚úÖ Reggae j√° completo (250/250) - pulando.\n",
            "‚úÖ Rhythm_blues j√° completo (250/250) - pulando.\n",
            "\n",
            "üîπ Rock_music: precisa baixar 1 arquivos (continuando ap√≥s √≠ndice 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] The provided YouTube account cookies are no longer valid. They have likely been rotated in the browser as a security measure. For tips on how to effectively export YouTube cookies, refer to  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies .\n",
            "WARNING: [youtube] The provided YouTube account cookies are no longer valid. They have likely been rotated in the browser as a security measure. For tips on how to effectively export YouTube cookies, refer to  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies .\n",
            "WARNING: [youtube] The provided YouTube account cookies are no longer valid. They have likely been rotated in the browser as a security measure. For tips on how to effectively export YouTube cookies, refer to  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies .\n",
            "WARNING: [youtube] The provided YouTube account cookies are no longer valid. They have likely been rotated in the browser as a security measure. For tips on how to effectively export YouTube cookies, refer to  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies .\n",
            "ERROR: [youtube] -08IuAXloCI: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -08IuAXloCI: ERROR: [youtube] -08IuAXloCI: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -5HSR5eWEDU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -5HSR5eWEDU: ERROR: [youtube] -5HSR5eWEDU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -82OBRkQskQ: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -82OBRkQskQ: ERROR: [youtube] -82OBRkQskQ: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -LTES9d6dZY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -LTES9d6dZY: ERROR: [youtube] -LTES9d6dZY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -TdKFIt-tlY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -TdKFIt-tlY: ERROR: [youtube] -TdKFIt-tlY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -XCNxDxsAL0: Video unavailable. This video is no longer available due to a copyright claim by Rock & Roll Hall of Fame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -XCNxDxsAL0: ERROR: [youtube] -XCNxDxsAL0: Video unavailable. This video is no longer available due to a copyright claim by Rock & Roll Hall of Fame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -aA0j_NIJcg: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -aA0j_NIJcg: ERROR: [youtube] -aA0j_NIJcg: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -fTW98nVe1g: Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -fTW98nVe1g: ERROR: [youtube] -fTW98nVe1g: Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -hrpU1nxsbc: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -hrpU1nxsbc: ERROR: [youtube] -hrpU1nxsbc: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -lAVvperqpQ: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -lAVvperqpQ: ERROR: [youtube] -lAVvperqpQ: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -pRIE4KkY1M: Video unavailable. This video is no longer available due to a copyright claim by Coda Publishing Ltd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -pRIE4KkY1M: ERROR: [youtube] -pRIE4KkY1M: Video unavailable. This video is no longer available due to a copyright claim by Coda Publishing Ltd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 061qiXuzrQs: Video unavailable. This video is no longer available due to a copyright claim by Studio Hamburg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 061qiXuzrQs: ERROR: [youtube] 061qiXuzrQs: Video unavailable. This video is no longer available due to a copyright claim by Studio Hamburg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0AQvRSZ8ZNo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0AQvRSZ8ZNo: ERROR: [youtube] 0AQvRSZ8ZNo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0H3VllGZQEU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0H3VllGZQEU: ERROR: [youtube] 0H3VllGZQEU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0Ma-YlsySuU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0Ma-YlsySuU: ERROR: [youtube] 0Ma-YlsySuU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0YRM3tkIXfA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0YRM3tkIXfA: ERROR: [youtube] 0YRM3tkIXfA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0q_EX7TiuV8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0q_EX7TiuV8: ERROR: [youtube] 0q_EX7TiuV8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0sMqbNNwzvU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0sMqbNNwzvU: ERROR: [youtube] 0sMqbNNwzvU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0sqcvMHyQ1k: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0sqcvMHyQ1k: ERROR: [youtube] 0sqcvMHyQ1k: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0z7EpFwegJM: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0z7EpFwegJM: ERROR: [youtube] 0z7EpFwegJM: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 1-Oahk8L_Og: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 1-Oahk8L_Og: ERROR: [youtube] 1-Oahk8L_Og: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 1A8piNIeg6w: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 1A8piNIeg6w: ERROR: [youtube] 1A8piNIeg6w: Video unavailable\n",
            "üéµ 250 baixados para Rock_music\n",
            "üéØ Rock_music atingiu 250, parando.\n",
            "‚úÖ Techno j√° completo (250/250) - pulando.\n",
            "\n",
            "üîπ Vocal: precisa baixar 16 arquivos (continuando ap√≥s √≠ndice 46199).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] piAVTNAiZto: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar piAVTNAiZto: ERROR: [youtube] piAVTNAiZto: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] pkcKUTIWoUA: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar pkcKUTIWoUA: ERROR: [youtube] pkcKUTIWoUA: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] povpzIiWNhA: Video unavailable. This video is no longer available due to a copyright claim by EG Productions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar povpzIiWNhA: ERROR: [youtube] povpzIiWNhA: Video unavailable. This video is no longer available due to a copyright claim by EG Productions\n",
            "üéµ 240 baixados para Vocal\n",
            "üéµ 250 baixados para Vocal\n",
            "üéØ Vocal atingiu 250, parando.\n",
            "\n",
            "‚úÖ Download finalizado.\n",
            " - Pop_music: 250/250\n",
            " - Hip_hop_music: 250/250\n",
            " - Rock_music: 250/250\n",
            " - Rhythm_blues: 250/250\n",
            " - Reggae: 250/250\n",
            " - Vocal: 250/250\n",
            " - Techno: 250/250\n",
            "Progresso salvo em dataset_summary_unit2.csv e df_success_unit2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# üîç Verifica√ß√£o e balanceamento dos arquivos WAV\n",
        "# --------------------------\n",
        "print(\"\\n=== Verificando e balanceando os arquivos WAV ===\")\n",
        "\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# listar todos os arquivos .wav\n",
        "wav_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "if not wav_files:\n",
        "    print(\"‚ö†Ô∏è Nenhum arquivo WAV encontrado em\", WAV_DIR)\n",
        "\n",
        "# 1Ô∏è‚É£ Contar arquivos por g√™nero\n",
        "wav_by_genre = defaultdict(list)\n",
        "for f in wav_files:\n",
        "    try:\n",
        "        genre = f.split(\"_\")[-1].replace(\".wav\", \"\")\n",
        "        wav_by_genre[genre].append(f)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(\"\\nüéµ Contagem inicial por g√™nero:\")\n",
        "for g, lst in wav_by_genre.items():\n",
        "    print(f\" - {g}: {len(lst)}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Identificar duplicados pelo ID do v√≠deo\n",
        "ids_seen = set()\n",
        "duplicates = []\n",
        "for f in wav_files:\n",
        "    vid_id = f.split(\"_\")[0]\n",
        "    if vid_id in ids_seen:\n",
        "        duplicates.append(f)\n",
        "    ids_seen.add(vid_id)\n",
        "\n",
        "# Remover duplicados\n",
        "for dup in duplicates:\n",
        "    try:\n",
        "        os.remove(os.path.join(WAV_DIR, dup))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(f\"\\nüîÅ Duplicados removidos: {len(duplicates)}\")\n",
        "\n",
        "# 3Ô∏è‚É£ Verificar e eliminar excessos (> TARGET_PER_CLASS)\n",
        "for genre, files in wav_by_genre.items():\n",
        "    # Atualizar lista ap√≥s remo√ß√£o\n",
        "    current_files = [f for f in os.listdir(WAV_DIR) if f.endswith(f\"_{genre}.wav\")]\n",
        "    if len(current_files) > TARGET_PER_CLASS:\n",
        "        excess = len(current_files) - TARGET_PER_CLASS\n",
        "        to_remove = random.sample(current_files, excess)\n",
        "        for f in to_remove:\n",
        "            os.remove(os.path.join(WAV_DIR, f))\n",
        "        print(f\"‚öñÔ∏è {genre}: removidos {excess} arquivos aleat√≥rios para balancear ({TARGET_PER_CLASS} restantes).\")\n",
        "\n",
        "# 4Ô∏è‚É£ Contagem final\n",
        "final_counts = Counter([f.split(\"_\")[-1].replace(\".wav\", \"\") for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")])\n",
        "print(\"\\n‚úÖ Contagem final por g√™nero:\")\n",
        "for g, c in final_counts.items():\n",
        "    print(f\" - {g}: {c}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlwXd557mSdU",
        "outputId": "e892e37e-1e28-4f24-ca25-20aab10678fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Verificando e balanceando os arquivos WAV ===\n",
            "\n",
            "üéµ Contagem inicial por g√™nero:\n",
            " - Reggae: 250\n",
            " - music: 250\n",
            " - blues: 250\n",
            " - Techno: 250\n",
            " - Vocal: 250\n",
            "\n",
            "üîÅ Duplicados removidos: 0\n",
            "\n",
            "‚úÖ Contagem final por g√™nero:\n",
            " - Reggae: 250\n",
            " - music: 250\n",
            " - blues: 250\n",
            " - Techno: 250\n",
            " - Vocal: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4 - Gerar os espectrogramas (MEL) - corrigido e incremental\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 4: Gerando espectrogramas MEL (uma pasta por g√™nero, incremental).\")\n",
        "\n",
        "import librosa, librosa.display\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# N√£o limpar o diret√≥rio (mant√©m espectrogramas j√° gerados)\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "# Lista de arquivos WAV\n",
        "wav_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "if not wav_files:\n",
        "    print(\"‚ö†Ô∏è Nenhum arquivo WAV encontrado em\", WAV_DIR)\n",
        "\n",
        "# Gera espectrograma apenas se ainda n√£o existir o arquivo .jpg correspondente\n",
        "for f in tqdm(wav_files, desc=\"Gerando espectrogramas\"):\n",
        "    try:\n",
        "        # G√™nero √© a parte depois do √∫ltimo \"_\"\n",
        "        parts = f.split(\"_\")\n",
        "        class_name = parts[-1].replace(\".wav\", \"\")\n",
        "        class_dir = os.path.join(IMG_DIR, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        # Caminho de sa√≠da do espectrograma\n",
        "        output_path = os.path.join(class_dir, f.replace(\".wav\", \".jpg\"))\n",
        "        if os.path.exists(output_path):\n",
        "            continue  # pula se j√° existe\n",
        "\n",
        "        # Carregar o √°udio\n",
        "        y, sr = librosa.load(os.path.join(WAV_DIR, f), sr=SR)\n",
        "        y = np.append(y[0], y[1:] - 0.97*y[:-1])  # pre-emphasis\n",
        "\n",
        "        # Gerar espectrograma MEL (igual ao artigo original)\n",
        "        M = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048,\n",
        "                                           hop_length=512, n_mels=96, fmax=sr/2)\n",
        "        log_power = librosa.power_to_db(M, ref=np.max)\n",
        "\n",
        "        # Plotar e salvar imagem (sem eixos, fundo limpo)\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.axis(\"off\")\n",
        "        librosa.display.specshow(log_power, cmap=\"jet\", sr=sr, hop_length=512)\n",
        "        plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao processar\", f, \"‚Üí\", e)\n",
        "\n",
        "# Contar quantas imagens por g√™nero\n",
        "counts_img = {}\n",
        "for g in os.listdir(IMG_DIR):\n",
        "    gp = os.path.join(IMG_DIR, g)\n",
        "    if os.path.isdir(gp):\n",
        "        counts_img[g] = len([x for x in os.listdir(gp) if x.lower().endswith(\".jpg\")])\n",
        "\n",
        "# Salvar estat√≠sticas\n",
        "pd.DataFrame(list(counts_img.items()), columns=[\"genre\", \"images\"])\\\n",
        "  .to_csv(os.path.join(ROOT_DIR, \"image_counts_unit2.csv\"), index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Contagem de espectrogramas salva em image_counts_unit2.csv\")\n",
        "print(\"Resumo:\", counts_img)\n"
      ],
      "metadata": {
        "id": "03SuC-p7yueR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff882a3f-9fde-44b8-db6d-90f563e5105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etapa 4: Gerando espectrogramas MEL (uma pasta por g√™nero, incremental).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gerando espectrogramas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [03:19<00:00,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Contagem de espectrogramas salva em image_counts_unit2.csv\n",
            "Resumo: {'blues': 250, 'music': 250, 'Techno': 250, 'Vocal': 250, 'Reggae': 250}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# üîç Verifica√ß√£o e balanceamento dos espectrogramas\n",
        "# --------------------------\n",
        "print(\"\\n=== Verificando e balanceando os espectrogramas ===\")\n",
        "\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# 1Ô∏è‚É£ Contar arquivos por g√™nero\n",
        "jpg_by_genre = defaultdict(list)\n",
        "for genre_folder in os.listdir(IMG_DIR):\n",
        "    genre_path = os.path.join(IMG_DIR, genre_folder)\n",
        "    if not os.path.isdir(genre_path):\n",
        "        continue\n",
        "    for f in os.listdir(genre_path):\n",
        "        if f.endswith(\".jpg\"):\n",
        "            jpg_by_genre[genre_folder].append(f)\n",
        "\n",
        "print(\"\\nüñºÔ∏è Contagem inicial por g√™nero:\")\n",
        "for g, lst in jpg_by_genre.items():\n",
        "    print(f\" - {g}: {len(lst)}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Identificar e remover duplicados\n",
        "duplicates_jpg = []\n",
        "for genre, files in jpg_by_genre.items():\n",
        "    ids_seen = set()\n",
        "    for f in files:\n",
        "        vid_id = f.split(\"_\")[0]\n",
        "        if vid_id in ids_seen:\n",
        "            duplicates_jpg.append((genre, f))\n",
        "        ids_seen.add(vid_id)\n",
        "\n",
        "for genre, f in duplicates_jpg:\n",
        "    try:\n",
        "        os.remove(os.path.join(IMG_DIR, genre, f))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(f\"\\nüîÅ Duplicados removidos: {len(duplicates_jpg)}\")\n",
        "\n",
        "# 3Ô∏è‚É£ Se houver mais de TARGET_PER_CLASS, remover aleatoriamente\n",
        "for genre, files in jpg_by_genre.items():\n",
        "    current_files = [f for f in os.listdir(os.path.join(IMG_DIR, genre)) if f.endswith(\".jpg\")]\n",
        "    if len(current_files) > TARGET_PER_CLASS:\n",
        "        excess = len(current_files) - TARGET_PER_CLASS\n",
        "        to_remove = random.sample(current_files, excess)\n",
        "        for f in to_remove:\n",
        "            os.remove(os.path.join(IMG_DIR, genre, f))\n",
        "        print(f\"‚öñÔ∏è {genre}: removidos {excess} espectrogramas aleat√≥rios (ficaram {TARGET_PER_CLASS}).\")\n",
        "\n",
        "# 4Ô∏è‚É£ Contagem final\n",
        "final_counts_jpg = {}\n",
        "for genre in os.listdir(IMG_DIR):\n",
        "    genre_path = os.path.join(IMG_DIR, genre)\n",
        "    if os.path.isdir(genre_path):\n",
        "        count = len([f for f in os.listdir(genre_path) if f.endswith(\".jpg\")])\n",
        "        final_counts_jpg[genre] = count\n",
        "\n",
        "print(\"\\n‚úÖ Contagem final por g√™nero:\")\n",
        "for g, c in final_counts_jpg.items():\n",
        "    print(f\" - {g}: {c}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpdX2MeiTQvk",
        "outputId": "47b2e023-6c94-44d4-d949-d223dec0804a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Verificando e balanceando os espectrogramas ===\n",
            "\n",
            "üñºÔ∏è Contagem inicial por g√™nero:\n",
            " - blues: 250\n",
            " - music: 250\n",
            " - Techno: 250\n",
            " - Vocal: 250\n",
            " - Reggae: 250\n",
            "\n",
            "üîÅ Duplicados removidos: 0\n",
            "\n",
            "‚úÖ Contagem final por g√™nero:\n",
            " - blues: 250\n",
            " - music: 250\n",
            " - Techno: 250\n",
            " - Vocal: 250\n",
            " - Reggae: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 5 - Criar e treinar o Autoencoder Variacional (VAE) ‚Äì vers√£o FINAL compat√≠vel com TF 2.15 / Keras 3\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 5: Criando e treinando o Autoencoder Variacional (VAE) ‚Äì vers√£o FINAL compat√≠vel com TF 2.15 / Keras 3.\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# --------------------------\n",
        "# CONFIGURA√á√ïES\n",
        "# --------------------------\n",
        "latent_dim = LATENT_DIM\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "\n",
        "# caminhos salvos no formato moderno\n",
        "encoder_path = os.path.join(MODEL_DIR, \"encoder_unit2_final.keras\")\n",
        "decoder_path = os.path.join(MODEL_DIR, \"decoder_unit2_final.keras\")\n",
        "vae_weights_path = os.path.join(MODEL_DIR, \"vae_unit2_final.weights.h5\")\n",
        "\n",
        "# --------------------------\n",
        "# DADOS\n",
        "# --------------------------\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.1)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    IMG_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    IMG_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    subset='validation',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# ENCODER\n",
        "# --------------------------\n",
        "encoder_inputs = layers.Input(shape=input_shape, name=\"encoder_input\")\n",
        "x = layers.Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
        "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "# --------------------------\n",
        "# DECODER\n",
        "# --------------------------\n",
        "latent_inputs = layers.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
        "x = layers.Dense(28 * 28 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((28, 28, 64))(x)\n",
        "x = layers.Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = models.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n",
        "\n",
        "# --------------------------\n",
        "# CLASSE VAE PERSONALIZADA\n",
        "# --------------------------\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.recon_loss_tracker = tf.keras.metrics.Mean(name=\"recon_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def compute_losses(self, data):\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        bce = tf.keras.losses.binary_crossentropy(data, reconstruction)\n",
        "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(bce, axis=(1, 2)))\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
        "        )\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "        return total_loss, reconstruction_loss, kl_loss\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            total_loss, reconstruction_loss, kl_loss = self.compute_losses(data)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.recon_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"recon_loss\": self.recon_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        total_loss, reconstruction_loss, kl_loss = self.compute_losses(data)\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.recon_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"val_loss\": self.total_loss_tracker.result(),\n",
        "            \"val_recon_loss\": self.recon_loss_tracker.result(),\n",
        "            \"val_kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# --------------------------\n",
        "# COMPILAR E TREINAR\n",
        "# --------------------------\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "\n",
        "if not os.path.exists(vae_weights_path):\n",
        "    print(\"üß† Treinando VAE (vers√£o final compat√≠vel)...\")\n",
        "    vae.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=VAE_EPOCHS,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # ‚öôÔ∏è build antes de salvar pesos\n",
        "    vae.build((None, *input_shape))\n",
        "\n",
        "    # salvar nos formatos modernos\n",
        "    vae.encoder.save(encoder_path)\n",
        "    vae.decoder.save(decoder_path)\n",
        "    vae.save_weights(vae_weights_path)\n",
        "\n",
        "    print(\"‚úÖ VAE treinado e salvo com sucesso (.keras / .weights.h5)\")\n",
        "else:\n",
        "    print(\"‚öôÔ∏è Carregando pesos VAE existentes...\")\n",
        "    vae.build((None, *input_shape))\n",
        "    vae.encoder.load_weights(encoder_path, skip_mismatch=True)\n",
        "    vae.decoder.load_weights(decoder_path, skip_mismatch=True)\n",
        "    print(\"‚úÖ Modelos encoder/decoder carregados.\")\n"
      ],
      "metadata": {
        "id": "XYSAlGgQy2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5a5b611-c2fd-4d2b-8f98-994d226bdcbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etapa 5: Criando e treinando o Autoencoder Variacional (VAE) ‚Äì vers√£o FINAL compat√≠vel com TF 2.15 / Keras 3.\n",
            "Found 1125 images belonging to 5 classes.\n",
            "Found 125 images belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"encoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ encoder_input       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ \u001b[38;5;34m3\u001b[0m)                ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  ‚îÇ        \u001b[38;5;34m896\u001b[0m ‚îÇ encoder_input[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
              "‚îÇ                     ‚îÇ \u001b[38;5;34m32\u001b[0m)               ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    ‚îÇ     \u001b[38;5;34m18,496\u001b[0m ‚îÇ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îÇ                     ‚îÇ \u001b[38;5;34m64\u001b[0m)               ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    ‚îÇ     \u001b[38;5;34m73,856\u001b[0m ‚îÇ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îÇ                     ‚îÇ \u001b[38;5;34m128\u001b[0m)              ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)    ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_4 (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ \u001b[38;5;34m25,690,368\u001b[0m ‚îÇ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ z_mean (\u001b[38;5;33mDense\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ     \u001b[38;5;34m16,448\u001b[0m ‚îÇ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ z_log_var (\u001b[38;5;33mDense\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ     \u001b[38;5;34m16,448\u001b[0m ‚îÇ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ z (\u001b[38;5;33mLambda\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ z_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ z_log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ encoder_input       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> ‚îÇ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
              "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> ‚îÇ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> ‚îÇ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">25,690,368</span> ‚îÇ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> ‚îÇ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> ‚îÇ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ z (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ z_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ z_log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,816,512\u001b[0m (98.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,816,512</span> (98.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,816,512\u001b[0m (98.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,816,512</span> (98.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ z_sampling (\u001b[38;5;33mInputLayer\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50176\u001b[0m)          ‚îÇ     \u001b[38;5;34m3,261,440\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_8              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    ‚îÇ        \u001b[38;5;34m73,856\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_9              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   ‚îÇ        \u001b[38;5;34m73,792\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_10             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   ‚îÇ        \u001b[38;5;34m18,464\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_11             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    ‚îÇ           \u001b[38;5;34m867\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ z_sampling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50176</span>)          ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,261,440</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_8              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_9              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_10             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_transpose_11             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">867</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,428,419\u001b[0m (13.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,428,419</span> (13.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,428,419\u001b[0m (13.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,428,419</span> (13.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Treinando VAE (vers√£o final compat√≠vel)...\n",
            "Epoch 1/2\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 5s/step - kl_loss: 1.8715 - loss: 34718.9180 - recon_loss: 34717.0469 - val_val_kl_loss: 217.6734 - val_val_loss: 33878.2109 - val_val_recon_loss: 33660.5391\n",
            "Epoch 2/2\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 5s/step - kl_loss: 505.3818 - loss: 31762.4980 - recon_loss: 31257.1152 - val_val_kl_loss: 64.4288 - val_val_loss: 28218.8828 - val_val_recon_loss: 28154.4551\n",
            "‚úÖ VAE treinado e salvo com sucesso (.keras / .weights.h5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 6 - Criar augmenta√ß√µes (novas abordagens)\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 6: criando augmenta√ß√µes (reconstru√ß√µes VAE, interpola√ß√£o latente, SpecAugment, spectral mix)\")\n",
        "\n",
        "ensure_empty_dir(AUG_DIR)\n",
        "for g in sorted(os.listdir(IMG_DIR)):\n",
        "    os.makedirs(os.path.join(AUG_DIR, g), exist_ok=True)\n",
        "\n",
        "# 6A: Reconstru√ß√µes ruidosas (VAE reconstruction of noisy input)\n",
        "def reconstruct_with_vae(X, batch=32):\n",
        "    recs = vae.predict(X, batch_size=batch)\n",
        "    return recs\n",
        "\n",
        "# gerar reconstru√ß√µes para cada imagem de treino (uma por amostra)\n",
        "for i, p in enumerate(tqdm(train_paths)):\n",
        "    cls = train_y[i]\n",
        "    # carregar imagem\n",
        "    img = Image.open(os.path.join(IMG_DIR, p)).convert(\"RGB\").resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "    arr = np.array(img, dtype=np.float32)/255.0\n",
        "    noisy = add_noise(np.expand_dims(arr,0), NOISE_FACTOR)\n",
        "    rec = vae.predict(noisy)[0]\n",
        "    outname = os.path.splitext(os.path.basename(p))[0] + \"_vae_rec.jpg\"\n",
        "    Image.fromarray((rec*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6B: Interpola√ß√£o no espa√ßo latente (entre pares da mesma classe e entre classes similares)\n",
        "N_INTERP_PER_CLASS = 2\n",
        "# Criar encoder model (input image -> z_mean) para obter m√©dias latentes para interpola√ß√£o\n",
        "encoder_model = models.Model(inputs, z_mean)  # usar z_mean (determin√≠stico) para interpolar de forma est√°vel\n",
        "\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    if len(class_indices) < 2: continue\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 10), replace=False)\n",
        "    for k in range(N_INTERP_PER_CLASS):\n",
        "        a,b = np.random.choice(chosen, size=2, replace=False)\n",
        "        img_a = np.array(Image.open(os.path.join(IMG_DIR, train_paths[a])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        img_b = np.array(Image.open(os.path.join(IMG_DIR, train_paths[b])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        za = encoder_model.predict(img_a[np.newaxis,...])[0]\n",
        "        zb = encoder_model.predict(img_b[np.newaxis,...])[0]\n",
        "        alpha = np.random.uniform(0.25, 0.75)\n",
        "        zint = alpha * za + (1-alpha) * zb\n",
        "        rec = decoder.predict(zint[np.newaxis,...])[0]\n",
        "        outname = f\"interp_{k}_{os.path.basename(train_paths[a])}_{os.path.basename(train_paths[b])}.jpg\"\n",
        "        Image.fromarray((np.clip(rec,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6C: SpecAugment (aplicado diretamente nos espectrogramas antes de salvar)\n",
        "def spec_augment_image(img_arr, time_masks=SPEC_AUG_TIME_MASKS, freq_masks=SPEC_AUG_FREQ_MASKS, max_time_mask_pct=0.2, max_freq_mask_pct=0.15):\n",
        "    # img_arr in [0,1], shape (H,W,C) but SpecAugment manipulates spectral axis (H=n_mels) and time axis (W)\n",
        "    img = img_arr.copy()\n",
        "    H, W = img.shape[0], img.shape[1]\n",
        "    for _ in range(time_masks):\n",
        "        t = int(np.random.uniform(0.0, max_time_mask_pct) * W)\n",
        "        t0 = np.random.randint(0, max(1, W - t + 1))\n",
        "        img[:, t0:t0+t, :] = 0\n",
        "    for _ in range(freq_masks):\n",
        "        f = int(np.random.uniform(0.0, max_freq_mask_pct) * H)\n",
        "        f0 = np.random.randint(0, max(1, H - f + 1))\n",
        "        img[f0:f0+f, :, :] = 0\n",
        "    return img\n",
        "\n",
        "# Aplicar SpecAugment a algumas imagens de treino e salvar\n",
        "N_SPEC_AUG_PER_CLASS = 2\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 20), replace=False)\n",
        "    for i_idx in chosen[:N_SPEC_AUG_PER_CLASS]:\n",
        "        p = train_paths[i_idx]\n",
        "        arr = np.array(Image.open(os.path.join(IMG_DIR, p)).convert(\"RGB\").resize((IMG_WIDTH, IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        aug = spec_augment_image(arr)\n",
        "        outname = os.path.splitext(os.path.basename(p))[0] + \"_specaug.jpg\"\n",
        "        Image.fromarray((np.clip(aug,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6D: Spectral Mix (linear mix of two spectrogram images) - naive mix (hard label use original or duplicate)\n",
        "N_MIX_PER_CLASS = 2\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    if len(class_indices) < 2: continue\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 20), replace=False)\n",
        "    for k in range(N_MIX_PER_CLASS):\n",
        "        a,b = np.random.choice(chosen, size=2, replace=False)\n",
        "        arr_a = np.array(Image.open(os.path.join(IMG_DIR, train_paths[a])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        arr_b = np.array(Image.open(os.path.join(IMG_DIR, train_paths[b])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        alpha = np.random.uniform(0.3, 0.7)\n",
        "        mixed = alpha*arr_a + (1-alpha)*arr_b\n",
        "        outname = f\"mix_{k}_{os.path.basename(train_paths[a])}_{os.path.basename(train_paths[b])}.jpg\"\n",
        "        Image.fromarray((np.clip(mixed,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6E: finalmente, copiamos os originais para AUG_DIR (mant√©m originais + artificially generated)\n",
        "for p, y in zip(train_paths, train_y):\n",
        "    src = os.path.join(IMG_DIR, p)\n",
        "    dst = os.path.join(AUG_DIR, y, os.path.basename(p))\n",
        "    if not os.path.exists(dst):\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "# salvar resumo de quantos arquivos por classe no AUG_DIR\n",
        "aug_counts = {g: len([f for f in os.listdir(os.path.join(AUG_DIR,g)) if f.lower().endswith(\".jpg\")]) for g in os.listdir(AUG_DIR)}\n",
        "pd.DataFrame(list(aug_counts.items()), columns=[\"genre\",\"aug_images\"]).to_csv(os.path.join(ROOT_DIR, \"aug_image_counts_unit2.csv\"), index=False)\n",
        "print(\"Augmentation conclu√≠da. Counts saved to aug_image_counts_unit2.csv\")"
      ],
      "metadata": {
        "id": "bWP5r5F1y7uI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "e1225a53-53b6-47fc-f169-d82dbd91fde8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etapa 6: criando augmenta√ß√µes (reconstru√ß√µes VAE, interpola√ß√£o latente, SpecAugment, spectral mix)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_paths' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-281224329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# gerar reconstru√ß√µes para cada imagem de treino (uma por amostra)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# carregar imagem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_paths' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 7 - Testes: treinar 2 CNNs\n",
        "#   - CNN A: VGG16 transfer learning com dados originais (baseline)\n",
        "#   - CNN B: VGG16 transfer learning com dados originais + AUG_DIR\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 7: treinar CNNs baseline e augmented\")\n",
        "\n",
        "# Fun√ß√µes auxiliares para coletar arquivos\n",
        "def collect_files_labels(root):\n",
        "    files, labels = [], []\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        cls_dir = os.path.join(root, cls)\n",
        "        if not os.path.isdir(cls_dir): continue\n",
        "        for f in os.listdir(cls_dir):\n",
        "            if f.lower().endswith(\".jpg\"):\n",
        "                files.append(os.path.join(cls_dir, f))\n",
        "                labels.append(cls)\n",
        "    return files, labels\n",
        "\n",
        "orig_files, orig_labels = collect_files_labels(IMG_DIR)\n",
        "aug_files, aug_labels = collect_files_labels(AUG_DIR)\n",
        "\n",
        "# Criar splits (para compara√ß√£o justa manter o mesmo test set - usar test_paths criados anteriormente)\n",
        "# We'll use test_paths list (relative paths) to define test set file paths in IMG_DIR\n",
        "test_file_paths = [os.path.join(IMG_DIR, p) for p in test_paths]\n",
        "\n",
        "# Create function to split train/val for baseline using orig_files excluding test_files\n",
        "def split_files_for_training(files, labels, test_file_paths, val_frac=0.2):\n",
        "    # filter out test\n",
        "    files_filtered, labels_filtered = [], []\n",
        "    for f, l in zip(files, labels):\n",
        "        if os.path.abspath(f) in [os.path.abspath(x) for x in test_file_paths]:\n",
        "            continue\n",
        "        files_filtered.append(f); labels_filtered.append(l)\n",
        "    tr_files, val_files, tr_labels, val_labels = train_test_split(files_filtered, labels_filtered, test_size=val_frac, stratify=labels_filtered, random_state=RANDOM_STATE)\n",
        "    return tr_files, val_files, tr_labels, val_labels\n",
        "\n",
        "train_files_base, val_files_base, train_labels_base, val_labels_base = split_files_for_training(orig_files, orig_labels, test_file_paths)\n",
        "train_files_aug, val_files_aug, train_labels_aug, val_labels_aug = split_files_for_training(aug_files, aug_labels, test_file_paths)\n",
        "\n",
        "print(\"Baseline train size:\", len(train_files_base), \"val:\", len(val_files_base), \"test:\", len(test_file_paths))\n",
        "print(\"Augmented train size:\", len(train_files_aug), \"val:\", len(val_files_aug), \"test:\", len(test_file_paths))\n",
        "\n",
        "# Generator Keras Sequence\n",
        "from tensorflow.keras.utils import Sequence\n",
        "class ImageSequence(Sequence):\n",
        "    def __init__(self, files, labels, le, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n",
        "        self.files = files\n",
        "        self.labels = np.array(labels)\n",
        "        self.le = le\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indexes = np.arange(len(self.files))\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.files)/self.batch_size)\n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_files = [self.files[i] for i in batch_idx]\n",
        "        batch_labels = self.labels[batch_idx]\n",
        "        imgs = []\n",
        "        for p in batch_files:\n",
        "            img = Image.open(p).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT))\n",
        "            arr = np.array(img, dtype=np.float32)/255.0\n",
        "            if self.augment:\n",
        "                # apply random small transforms: horizontal flip, small brightness jitter, spec augment occasionally\n",
        "                if random.random() < 0.1:\n",
        "                    arr = np.fliplr(arr)\n",
        "                if random.random() < 0.2:\n",
        "                    arr = np.clip(arr + 0.03 * np.random.randn(*arr.shape), 0, 1)\n",
        "            imgs.append(arr)\n",
        "        X = np.stack(imgs)\n",
        "        y = to_categorical(self.le.transform(batch_labels), num_classes=len(self.le.classes_))\n",
        "        return X, y\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "# label encoder based on classes found\n",
        "le_classes = LabelEncoder()\n",
        "le_classes.fit(sorted(os.listdir(IMG_DIR)))\n",
        "\n",
        "# Create generators\n",
        "train_gen_base = ImageSequence(train_files_base, train_labels_base, le_classes, batch_size=BATCH_SIZE, shuffle=True, augment=False)\n",
        "val_gen_base = ImageSequence(val_files_base, val_labels_base, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "train_gen_aug = ImageSequence(train_files_aug, train_labels_aug, le_classes, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
        "val_gen_aug = ImageSequence(val_files_aug, val_labels_aug, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "test_gen = ImageSequence(test_file_paths, test_y, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "\n",
        "# Build VGG16 transfer model factory\n",
        "def build_vgg_transfer(num_classes, train_base=False):\n",
        "    base = VGG16(include_top=False, weights='imagenet', input_shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "    base.trainable = train_base\n",
        "    inp = layers.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "num_classes = len(le_classes.classes_)\n",
        "print(\"Num classes:\", num_classes)\n",
        "\n",
        "# Train baseline\n",
        "print(\"Treinando VGG16 baseline (originais)...\")\n",
        "vgg_base = build_vgg_transfer(num_classes, train_base=False)\n",
        "es = callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
        "history_base = vgg_base.fit(train_gen_base, epochs=CLASS_EPOCHS, validation_data=val_gen_base, callbacks=[es])\n",
        "vgg_base.save(os.path.join(MODEL_DIR, \"vgg_base_unit2.h5\"))\n",
        "\n",
        "# Evaluate baseline\n",
        "def evaluate_keras_model(model, seq):\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "    for Xb, yb in seq:\n",
        "        probs = model.predict(Xb)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_prob.extend(probs.tolist())\n",
        "        y_true.extend(np.argmax(yb, axis=1).tolist())\n",
        "    return np.array(y_true), np.array(y_pred), np.array(y_prob)\n",
        "\n",
        "y_true_base, y_pred_base, y_prob_base = evaluate_keras_model(vgg_base, test_gen)\n",
        "acc_base = accuracy_score(y_true_base, y_pred_base)\n",
        "f1_base = f1_score(y_true_base, y_pred_base, average='macro')\n",
        "print(\"Baseline: Acc\", acc_base, \"F1\", f1_base)\n",
        "\n",
        "# Train augmented\n",
        "print(\"Treinando VGG16 com dataset aumentado...\")\n",
        "vgg_aug = build_vgg_transfer(num_classes, train_base=False)\n",
        "history_aug = vgg_aug.fit(train_gen_aug, epochs=CLASS_EPOCHS, validation_data=val_gen_aug, callbacks=[es])\n",
        "vgg_aug.save(os.path.join(MODEL_DIR, \"vgg_aug_unit2.h5\"))\n",
        "\n",
        "y_true_aug, y_pred_aug, y_prob_aug = evaluate_keras_model(vgg_aug, test_gen)\n",
        "acc_aug = accuracy_score(y_true_aug, y_pred_aug)\n",
        "f1_aug = f1_score(y_true_aug, y_pred_aug, average='macro')\n",
        "print(\"Augmented: Acc\", acc_aug, \"F1\", f1_aug)"
      ],
      "metadata": {
        "id": "XMUlI35sy_tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 8 - Resultados: tabela, ROC e matrizes de confus√£o\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 8: Gerando m√©tricas, curvas ROC e matrizes de confus√£o (sem PDF).\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Cria√ß√£o da tabela de m√©tricas\n",
        "y_test_bin = label_binarize(y_true_base, classes=np.arange(num_classes))\n",
        "try:\n",
        "    auc_base = roc_auc_score(y_test_bin, y_prob_base, average='macro')\n",
        "except Exception:\n",
        "    auc_base = np.nan\n",
        "try:\n",
        "    auc_aug = roc_auc_score(y_test_bin, y_prob_aug, average='macro')\n",
        "except Exception:\n",
        "    auc_aug = np.nan\n",
        "\n",
        "metrics_df = pd.DataFrame([\n",
        "    [\"VGG16 (baseline)\", acc_base, f1_base, auc_base],\n",
        "    [\"VGG16 (augmented)\", acc_aug, f1_aug, auc_aug]\n",
        "], columns=[\"Model\", \"Accuracy\", \"F1_macro\", \"AUC_macro\"])\n",
        "\n",
        "# Salvar tabela\n",
        "metrics_path = os.path.join(RESULTS_DIR, \"metrics_comparison_unit2.csv\")\n",
        "metrics_df.to_csv(metrics_path, index=False)\n",
        "print(\"\\nüìä Tabela de m√©tricas:\")\n",
        "print(metrics_df)\n",
        "print(f\"\\nTabela salva em: {metrics_path}\")\n",
        "\n",
        "# --------------------------\n",
        "# Curvas ROC\n",
        "# --------------------------\n",
        "def multiclass_roc(y_true, y_prob, n_classes):\n",
        "    y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    for i in range(n_classes):\n",
        "        try:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], np.array(y_prob)[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        except:\n",
        "            fpr[i], tpr[i], roc_auc[i] = None, None, None\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes) if fpr[i] is not None]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        if fpr[i] is not None:\n",
        "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"], tpr[\"macro\"], roc_auc[\"macro\"] = all_fpr, mean_tpr, auc(all_fpr, mean_tpr)\n",
        "    return fpr, tpr, roc_auc\n",
        "\n",
        "fpr_b, tpr_b, roc_auc_b = multiclass_roc(y_true_base, y_prob_base, num_classes)\n",
        "fpr_a, tpr_a, roc_auc_a = multiclass_roc(y_true_aug, y_prob_aug, num_classes)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr_b[\"macro\"], tpr_b[\"macro\"], '--', label=f'Baseline (AUC={roc_auc_b[\"macro\"]:.3f})')\n",
        "plt.plot(fpr_a[\"macro\"], tpr_a[\"macro\"], '-.', label=f'Augmented (AUC={roc_auc_a[\"macro\"]:.3f})')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Compara√ß√£o (Macro Average)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "roc_path = os.path.join(RESULTS_DIR, \"roc_comparison.png\")\n",
        "plt.savefig(roc_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"‚úÖ Curva ROC salva em: {roc_path}\")\n",
        "\n",
        "# --------------------------\n",
        "# Matrizes de confus√£o\n",
        "# --------------------------\n",
        "cm_base = confusion_matrix(y_true_base, y_pred_base)\n",
        "cm_aug  = confusion_matrix(y_true_aug, y_pred_aug)\n",
        "\n",
        "# Baseline\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm_base, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=le_classes.classes_, yticklabels=le_classes.classes_)\n",
        "plt.title(\"Matriz de Confus√£o - CNN Base\")\n",
        "plt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\n",
        "cm_base_path = os.path.join(RESULTS_DIR, \"cm_baseline.png\")\n",
        "plt.savefig(cm_base_path, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# Augmented\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm_aug, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=le_classes.classes_, yticklabels=le_classes.classes_)\n",
        "plt.title(\"Matriz de Confus√£o - CNN com Augmentation\")\n",
        "plt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\n",
        "cm_aug_path = os.path.join(RESULTS_DIR, \"cm_augmented.png\")\n",
        "plt.savefig(cm_aug_path, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(f\"‚úÖ Matrizes salvas em:\\n - {cm_base_path}\\n - {cm_aug_path}\")\n",
        "\n",
        "print(\"\\nüéØ Resultados finais conclu√≠dos! Verifique os arquivos na pasta 'results_unit2'\")\n"
      ],
      "metadata": {
        "id": "SOjhLBn401Pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}