{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yt_dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TelpVnVcZbg",
        "outputId": "754d1d69-4431-431b-e6e1-a58585cbd20d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2025.10.14-py3-none-any.whl.metadata (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.9/175.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.10.14-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp\n",
            "Successfully installed yt_dlp-2025.10.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5CfscjXKvl8l"
      },
      "outputs": [],
      "source": [
        "# --------------------------\n",
        "# 1 - installs, imports e diretórios (ATUALIZADO)\n",
        "# --------------------------\n",
        "# se rodando em notebook: descomente a linha abaixo para garantir libs\n",
        "# !pip install numpy pandas matplotlib seaborn tqdm pillow librosa audioread yt-dlp ffmpeg-python scikit-learn xgboost tensorflow\n",
        "\n",
        "import os, re, shutil, subprocess, math, random, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa, librosa.display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K, callbacks, optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "\n",
        "# Fixar seeds\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --------------------------\n",
        "# Diretórios principais (ajuste conforme seu Drive)\n",
        "# --------------------------\n",
        "\n",
        "ROOT_DIR = \"/content/drive/MyDrive/M.L_2UNIDADE\"   # caminho principal do projeto da 2ª unidade\n",
        "\n",
        "# Estrutura de pastas\n",
        "CSV_DIR     = os.path.join(ROOT_DIR, \"csv_files\")              # arquivos CSV do dataset\n",
        "COOKIES_DIR = os.path.join(ROOT_DIR, \"cookies\")                # cookies yt-dlp (opcional)\n",
        "WAV_DIR     = os.path.join(ROOT_DIR, \"wav_files\")              # cortes de áudio WAV (10s)\n",
        "IMG_DIR     = os.path.join(ROOT_DIR, \"data-files\")             # espectrogramas originais\n",
        "AUG_DIR     = os.path.join(ROOT_DIR, \"data-files-augmented\")   # espectrogramas com augmentations\n",
        "MODEL_DIR   = os.path.join(ROOT_DIR, \"models_unit2\")           # modelos treinados (VAE, CNNs)\n",
        "RESULTS_DIR = os.path.join(ROOT_DIR, \"results_unit2\")          # métricas, curvas ROC, matrizes de confusão\n",
        "\n",
        "# Criar diretórios caso não existam\n",
        "for d in [CSV_DIR, COOKIES_DIR, WAV_DIR, IMG_DIR, AUG_DIR, MODEL_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Caminhos específicos\n",
        "CSV_PATH     = os.path.join(CSV_DIR, \"unbalanced_train_segments.csv\")  # caminho do CSV principal\n",
        "COOKIES_PATH = os.path.join(COOKIES_DIR, \"www.youtube.com_cookies.txt\")                 # cookies yt-dlp (se necessário)\n",
        "\n",
        "# --------------------------\n",
        "# Parâmetros principais\n",
        "# --------------------------\n",
        "TARGET_PER_CLASS = 250    # número de músicas por gênero\n",
        "SR = 22050                # sample rate\n",
        "CLIP_DURATION = 10        # duração dos clipes em segundos\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "VAE_EPOCHS = 60\n",
        "CLASS_EPOCHS = 15\n",
        "LATENT_DIM = 64\n",
        "NOISE_FACTOR = 0.08\n",
        "SPEC_AUG_TIME_MASKS = 2\n",
        "SPEC_AUG_FREQ_MASKS = 2\n",
        "\n",
        "RANDOM_STATE = SEED\n",
        "\n",
        "# --------------------------\n",
        "# Gêneros alvo (iguais ao código original; pode ajustar se quiser)\n",
        "# --------------------------\n",
        "genre_dict = {\n",
        "    '/m/064t9': 'Pop_music',\n",
        "    '/m/0glt670': 'Hip_hop_music',\n",
        "    '/m/06by7': 'Rock_music',\n",
        "    '/m/06j6l': 'Rhythm_blues',\n",
        "    '/m/06cqb': 'Reggae',\n",
        "    '/m/0y4f8': 'Vocal',\n",
        "    '/m/07gxw': 'Techno',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Helpers utilitários\n",
        "# --------------------------\n",
        "def ensure_empty_dir(d):\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def plot_and_save_fig(fig, filename):\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(filename, dpi=200)\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "REzTtoBDyTop"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 2 - carregamento do dataset (leitura do CSV e filtragem por gêneros)\n",
        "# --------------------------\n",
        "print(\"Etapa 2: lendo CSV e preparando candidatos...\")\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"CSV não encontrado em {CSV_PATH} - verifique caminho\")\n",
        "\n",
        "data = []\n",
        "with open(CSV_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "        elements = re.sub(r'[\"\\n]', \"\", line).split(\",\")\n",
        "        if len(elements) >= 4:\n",
        "            url = elements[0]; start = elements[1]; end = elements[2]; labels = elements[3:]\n",
        "            for label in labels:\n",
        "                if label in genre_dict:\n",
        "                    data.append([url, start, end, genre_dict[label]])\n",
        "df = pd.DataFrame(data, columns=[\"url\", \"start_time\", \"end_time\", \"class_label\"])\n",
        "df.to_csv(os.path.join(ROOT_DIR, \"df_candidates_unit2.csv\"), index=False)\n",
        "print(\"Total candidatos lidos:\", len(df))\n",
        "print(df[\"class_label\"].value_counts())"
      ],
      "metadata": {
        "id": "MN4zR-23yXin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23f1077-7cc0-4b6d-d820-6e17fb7b1272"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etapa 2: lendo CSV e preparando candidatos...\n",
            "Total candidatos lidos: 51710\n",
            "class_label\n",
            "Techno           16811\n",
            "Pop_music         8407\n",
            "Rock_music        8198\n",
            "Hip_hop_music     7370\n",
            "Rhythm_blues      4755\n",
            "Vocal             3241\n",
            "Reggae            2928\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 3 - corte dos vídeos (download e cut) — com retomada ordenada\n",
        "# --------------------------\n",
        "COOKIES_PATH = os.path.join(COOKIES_DIR, \"www.youtube.com_cookies.txt\")\n",
        "print(\"\\nEtapa 3: Download e corte dos vídeos (com retomada ordenada e verificação incremental).\")\n",
        "\n",
        "import yt_dlp\n",
        "import pandas as pd\n",
        "import os, subprocess\n",
        "\n",
        "# 🔸 NÃO limpar o diretório\n",
        "os.makedirs(WAV_DIR, exist_ok=True)\n",
        "\n",
        "# 🔸 Verificar progresso anterior\n",
        "progress_file = os.path.join(ROOT_DIR, \"df_success_unit2.csv\")\n",
        "if os.path.exists(progress_file):\n",
        "    df_success = pd.read_csv(progress_file)\n",
        "    downloaded_ids = set(df_success[\"url\"].astype(str))\n",
        "    print(f\"✅ Retomando: {len(downloaded_ids)} vídeos já baixados com sucesso.\")\n",
        "else:\n",
        "    df_success = pd.DataFrame(columns=[\"url\", \"start_time\", \"end_time\", \"class_label\"])\n",
        "    downloaded_ids = set()\n",
        "\n",
        "# 🔸 Contar quantos arquivos WAV já existem por gênero\n",
        "existing_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "existing_counts = {}\n",
        "for f in existing_files:\n",
        "    try:\n",
        "        genre_name = f.split(\"_\", 1)[1][:-4]\n",
        "        existing_counts[genre_name] = existing_counts.get(genre_name, 0) + 1\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "counts = {label: existing_counts.get(label, 0) for label in genre_dict.values()}\n",
        "print(\"📊 Arquivos já existentes por gênero:\", counts)\n",
        "\n",
        "# 🔸 Determinar o último vídeo baixado com sucesso (para cada gênero)\n",
        "last_positions = {}\n",
        "if not df_success.empty:\n",
        "    for genre in df_success[\"class_label\"].unique():\n",
        "        last_url = df_success[df_success[\"class_label\"] == genre][\"url\"].iloc[-1]\n",
        "        try:\n",
        "            pos = df[df[\"url\"] == last_url].index[-1]\n",
        "            last_positions[genre] = pos\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "# 🔸 Função auxiliar: verificar se já existe WAV\n",
        "def wav_exists(video_id, label):\n",
        "    pattern = f\"{video_id}_{label}.wav\"\n",
        "    return any(pattern in f for f in existing_files)\n",
        "\n",
        "# 🔸 Baixar somente os gêneros incompletos, continuando do último baixado\n",
        "downloaded_rows = []\n",
        "for label, group in df.groupby(\"class_label\"):\n",
        "    current_count = counts.get(label, 0)\n",
        "    if current_count >= TARGET_PER_CLASS:\n",
        "        print(f\"✅ {label} já completo ({current_count}/{TARGET_PER_CLASS}) - pulando.\")\n",
        "        continue\n",
        "\n",
        "    # se houver posição salva, começa dali\n",
        "    start_index = last_positions.get(label, group.index[0])\n",
        "    group = group.loc[group.index >= start_index]\n",
        "\n",
        "    needed = TARGET_PER_CLASS - current_count\n",
        "    print(f\"\\n🔹 {label}: precisa baixar {needed} arquivos (continuando após índice {start_index}).\")\n",
        "\n",
        "    for idx, row in group.iterrows():\n",
        "        vid = str(row[\"url\"])\n",
        "        start = float(row[\"start_time\"])\n",
        "        url = f\"https://www.youtube.com/watch?v={vid}\"\n",
        "        final_file = os.path.join(WAV_DIR, f\"{vid}_{label}.wav\")\n",
        "\n",
        "        # já baixado? pula\n",
        "        if vid in downloaded_ids or wav_exists(vid, label):\n",
        "            continue\n",
        "        if counts[label] >= TARGET_PER_CLASS:\n",
        "            print(f\"🎯 {label} atingiu {TARGET_PER_CLASS}, parando.\")\n",
        "            break\n",
        "\n",
        "        temp_out = os.path.join(WAV_DIR, f\"temp_{vid}.%(ext)s\")\n",
        "        try:\n",
        "            ydl_opts = {\n",
        "                \"format\": \"bestaudio/best\",\n",
        "                \"outtmpl\": temp_out,\n",
        "                \"quiet\": True,\n",
        "                \"noplaylist\": True\n",
        "            }\n",
        "            if os.path.exists(COOKIES_PATH):\n",
        "                ydl_opts[\"cookiefile\"] = COOKIES_PATH\n",
        "\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([url])\n",
        "\n",
        "            temp_files = [f for f in os.listdir(WAV_DIR) if f.startswith(f\"temp_{vid}\")]\n",
        "            if not temp_files:\n",
        "                print(f\"⚠️ Nenhum arquivo temporário gerado para {vid}\")\n",
        "                continue\n",
        "\n",
        "            temp_file = os.path.join(WAV_DIR, temp_files[0])\n",
        "            subprocess.run([\n",
        "                \"ffmpeg\", \"-y\", \"-ss\", str(start), \"-t\", str(CLIP_DURATION),\n",
        "                \"-i\", temp_file, \"-acodec\", \"pcm_s16le\", \"-ar\", str(SR), \"-ac\", \"1\", final_file\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            os.remove(temp_file)\n",
        "            if os.path.exists(final_file):\n",
        "                counts[label] += 1\n",
        "                downloaded_rows.append(row)\n",
        "                downloaded_ids.add(vid)\n",
        "\n",
        "                # salvar progresso incremental\n",
        "                pd.concat([df_success, pd.DataFrame([row])]).to_csv(progress_file, index=False)\n",
        "\n",
        "                if counts[label] % 10 == 0:\n",
        "                    print(f\"🎵 {counts[label]} baixados para {label}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"⚠️ WAV não gerado para {vid}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro ao baixar {vid}: {e}\")\n",
        "            continue\n",
        "\n",
        "# 🔸 Salvar resumo\n",
        "summary = pd.DataFrame(list(counts.items()), columns=[\"genre\", \"count\"])\n",
        "summary.to_csv(os.path.join(ROOT_DIR, \"dataset_summary_unit2.csv\"), index=False)\n",
        "\n",
        "print(\"\\n✅ Download finalizado.\")\n",
        "for g, c in counts.items():\n",
        "    print(f\" - {g}: {c}/{TARGET_PER_CLASS}\")\n",
        "print(\"Progresso salvo em dataset_summary_unit2.csv e df_success_unit2.csv\")\n"
      ],
      "metadata": {
        "id": "VJ2sofWSyawI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290f457f-0b72-46b6-be9d-2e123d5e3125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etapa 3: Download e corte dos vídeos (com retomada ordenada e verificação incremental).\n",
            "✅ Retomando: 10 vídeos já baixados com sucesso.\n",
            "📊 Arquivos já existentes por gênero: {'Pop_music': 250, 'Hip_hop_music': 250, 'Rock_music': 249, 'Rhythm_blues': 250, 'Reggae': 250, 'Vocal': 145, 'Techno': 250}\n",
            "✅ Hip_hop_music já completo (250/250) - pulando.\n",
            "✅ Pop_music já completo (250/250) - pulando.\n",
            "✅ Reggae já completo (250/250) - pulando.\n",
            "✅ Rhythm_blues já completo (250/250) - pulando.\n",
            "\n",
            "🔹 Rock_music: precisa baixar 1 arquivos (continuando após índice 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -08IuAXloCI: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -08IuAXloCI: ERROR: [youtube] -08IuAXloCI: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -5HSR5eWEDU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -5HSR5eWEDU: ERROR: [youtube] -5HSR5eWEDU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -82OBRkQskQ: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -82OBRkQskQ: ERROR: [youtube] -82OBRkQskQ: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -LTES9d6dZY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -LTES9d6dZY: ERROR: [youtube] -LTES9d6dZY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -TdKFIt-tlY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -TdKFIt-tlY: ERROR: [youtube] -TdKFIt-tlY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -XCNxDxsAL0: Video unavailable. This video is no longer available due to a copyright claim by Rock & Roll Hall of Fame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -XCNxDxsAL0: ERROR: [youtube] -XCNxDxsAL0: Video unavailable. This video is no longer available due to a copyright claim by Rock & Roll Hall of Fame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -aA0j_NIJcg: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -aA0j_NIJcg: ERROR: [youtube] -aA0j_NIJcg: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -fTW98nVe1g: Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -fTW98nVe1g: ERROR: [youtube] -fTW98nVe1g: Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -hrpU1nxsbc: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -hrpU1nxsbc: ERROR: [youtube] -hrpU1nxsbc: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -lAVvperqpQ: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -lAVvperqpQ: ERROR: [youtube] -lAVvperqpQ: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -pRIE4KkY1M: Video unavailable. This video is no longer available due to a copyright claim by Coda Publishing Ltd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar -pRIE4KkY1M: ERROR: [youtube] -pRIE4KkY1M: Video unavailable. This video is no longer available due to a copyright claim by Coda Publishing Ltd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 061qiXuzrQs: Video unavailable. This video is no longer available due to a copyright claim by Studio Hamburg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 061qiXuzrQs: ERROR: [youtube] 061qiXuzrQs: Video unavailable. This video is no longer available due to a copyright claim by Studio Hamburg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0AQvRSZ8ZNo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0AQvRSZ8ZNo: ERROR: [youtube] 0AQvRSZ8ZNo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0H3VllGZQEU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0H3VllGZQEU: ERROR: [youtube] 0H3VllGZQEU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0Ma-YlsySuU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0Ma-YlsySuU: ERROR: [youtube] 0Ma-YlsySuU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0YRM3tkIXfA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0YRM3tkIXfA: ERROR: [youtube] 0YRM3tkIXfA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0q_EX7TiuV8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0q_EX7TiuV8: ERROR: [youtube] 0q_EX7TiuV8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0sMqbNNwzvU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0sMqbNNwzvU: ERROR: [youtube] 0sMqbNNwzvU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0sqcvMHyQ1k: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0sqcvMHyQ1k: ERROR: [youtube] 0sqcvMHyQ1k: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0z7EpFwegJM: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 0z7EpFwegJM: ERROR: [youtube] 0z7EpFwegJM: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 1-Oahk8L_Og: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 1-Oahk8L_Og: ERROR: [youtube] 1-Oahk8L_Og: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 1A8piNIeg6w: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar 1A8piNIeg6w: ERROR: [youtube] 1A8piNIeg6w: Video unavailable\n",
            "🎵 250 baixados para Rock_music\n",
            "🎯 Rock_music atingiu 250, parando.\n",
            "✅ Techno já completo (250/250) - pulando.\n",
            "\n",
            "🔹 Vocal: precisa baixar 105 arquivos (continuando após índice 44967).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nI59vkjjZY0: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar nI59vkjjZY0: ERROR: [youtube] nI59vkjjZY0: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nIA8YdQGM1Y: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar nIA8YdQGM1Y: ERROR: [youtube] nIA8YdQGM1Y: Video unavailable. This video is private\n",
            "🎵 150 baixados para Vocal\n",
            "🎵 160 baixados para Vocal\n",
            "🎵 170 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nf7AArmXhys: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar nf7AArmXhys: ERROR: [youtube] nf7AArmXhys: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] npUieylloJE: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar npUieylloJE: ERROR: [youtube] npUieylloJE: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nqO5FTTuCvA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar nqO5FTTuCvA: ERROR: [youtube] nqO5FTTuCvA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
            "🎵 180 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nzldgEJxVds: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar nzldgEJxVds: ERROR: [youtube] nzldgEJxVds: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] o8Kc4i9YuZ4: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar o8Kc4i9YuZ4: ERROR: [youtube] o8Kc4i9YuZ4: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
            "🎵 190 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] oIyj9aDz74A: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar oIyj9aDz74A: ERROR: [youtube] oIyj9aDz74A: Video unavailable. This video is private\n",
            "🎵 200 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] oiWZWUQx_nY: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar oiWZWUQx_nY: ERROR: [youtube] oiWZWUQx_nY: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] ojVK0Tf2Udk: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar ojVK0Tf2Udk: ERROR: [youtube] ojVK0Tf2Udk: Video unavailable\n",
            "🎵 210 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] olxNrhVurkI: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro ao baixar olxNrhVurkI: ERROR: [youtube] olxNrhVurkI: Video unavailable. This video is private\n",
            "🎵 220 baixados para Vocal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4 - gerar os espectrogramas (MEL)\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 4: gerando espectrogramas MEL (224x224)...\")\n",
        "ensure_empty_dir(IMG_DIR)\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "wav_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "for f in tqdm(wav_files):\n",
        "    try:\n",
        "        class_name = f.split(\"_\",1)[1][:-4]\n",
        "        class_dir = os.path.join(IMG_DIR, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        y, sr = librosa.load(os.path.join(WAV_DIR, f), sr=SR)\n",
        "        # pre-emphasis\n",
        "        y = np.append(y[0], y[1:] - 0.97*y[:-1])\n",
        "        M = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128, fmax=sr/2)\n",
        "        log_power = librosa.power_to_db(M, ref=np.max)\n",
        "        # plot\n",
        "        plt.figure(figsize=(3,3))\n",
        "        plt.axis('off')\n",
        "        librosa.display.specshow(log_power, sr=sr, hop_length=512, cmap='jet')\n",
        "        savepath = os.path.join(class_dir, f[:-4] + \".jpg\")\n",
        "        plt.savefig(savepath, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao processar\", f, e)\n",
        "\n",
        "# contar imagens por genero e salvar\n",
        "counts_img = {}\n",
        "for g in os.listdir(IMG_DIR):\n",
        "    gp = os.path.join(IMG_DIR, g)\n",
        "    if os.path.isdir(gp):\n",
        "        counts_img[g] = len([x for x in os.listdir(gp) if x.lower().endswith(\".jpg\")])\n",
        "pd.DataFrame(list(counts_img.items()), columns=[\"genre\",\"images\"]).to_csv(os.path.join(ROOT_DIR, \"image_counts_unit2.csv\"), index=False)\n",
        "print(\"Contagem de espectrogramas salva em image_counts_unit2.csv\")"
      ],
      "metadata": {
        "id": "03SuC-p7yueR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 5 - criar e treinar o autoencoder (VAE conv) - novidade\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 5: construindo e treinando VAE conv (denoising VAE).\")\n",
        "\n",
        "# carregando imagens de treino (tudo por enquanto, separaremos depois)\n",
        "def load_images_by_class(img_root, resize=(IMG_WIDTH, IMG_HEIGHT), max_per_class=None):\n",
        "    X, y, paths = [], [], []\n",
        "    for cls in sorted(os.listdir(img_root)):\n",
        "        cls_dir = os.path.join(img_root, cls)\n",
        "        if not os.path.isdir(cls_dir): continue\n",
        "        files = sorted([f for f in os.listdir(cls_dir) if f.lower().endswith(\".jpg\")])\n",
        "        if max_per_class:\n",
        "            files = files[:max_per_class]\n",
        "        for f in files:\n",
        "            img = Image.open(os.path.join(cls_dir,f)).convert(\"RGB\").resize(resize)\n",
        "            arr = np.array(img, dtype=np.float32)/255.0\n",
        "            X.append(arr)\n",
        "            y.append(cls)\n",
        "            paths.append(os.path.join(cls, f))\n",
        "    return np.stack(X), np.array(y), paths\n",
        "\n",
        "# Carrega tudo (atenção memória)\n",
        "X_all, y_all, paths_all = load_images_by_class(IMG_DIR, max_per_class=None)\n",
        "print(\"Total imagens espectrograma:\", X_all.shape)\n",
        "\n",
        "# separar treino/val/test (estratificado por genero)\n",
        "train_X, test_X, train_y, test_y, train_paths, test_paths = train_test_split(\n",
        "    X_all, y_all, paths_all, test_size=0.2, stratify=y_all, random_state=RANDOM_STATE)\n",
        "train_X, val_X, train_y, val_y, train_paths, val_paths = train_test_split(\n",
        "    train_X, train_y, train_paths, test_size=0.2, stratify=train_y, random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"Splits -> train:\", train_X.shape[0], \"val:\", val_X.shape[0], \"test:\", test_X.shape[0])\n",
        "\n",
        "# VAE model (encoder + sampling + decoder)\n",
        "input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "# encoder\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "x = layers.MaxPooling2D(2, padding='same')(x)           # 112x112\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D(2, padding='same')(x)           # 56x56\n",
        "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D(2, padding='same')(x)           # 28x28\n",
        "shape_before_flatten = K.int_shape(x)[1:]               # (28,28,128)\n",
        "flat = layers.Flatten()(x)\n",
        "h = layers.Dense(512, activation='relu')(flat)\n",
        "\n",
        "z_mean = layers.Dense(LATENT_DIM, name='z_mean')(h)\n",
        "z_log_var = layers.Dense(LATENT_DIM, name='z_log_var')(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], LATENT_DIM), mean=0., stddev=1.0)\n",
        "    return z_mean + K.exp(0.5*z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling, output_shape=(LATENT_DIM,))([z_mean, z_log_var])\n",
        "\n",
        "# decoder\n",
        "decoder_input = layers.Input(shape=(LATENT_DIM,))\n",
        "d = layers.Dense(np.prod(shape_before_flatten), activation='relu')(decoder_input)\n",
        "d = layers.Reshape(shape_before_flatten)(d)\n",
        "d = layers.Conv2D(128, 3, activation='relu', padding='same')(d)\n",
        "d = layers.UpSampling2D(2)(d)  # 56\n",
        "d = layers.Conv2D(64, 3, activation='relu', padding='same')(d)\n",
        "d = layers.UpSampling2D(2)(d)  # 112\n",
        "d = layers.Conv2D(32, 3, activation='relu', padding='same')(d)\n",
        "d = layers.UpSampling2D(2)(d)  # 224\n",
        "d = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(d)\n",
        "decoder = models.Model(decoder_input, d, name='decoder_model')\n",
        "\n",
        "# full VAE model\n",
        "decoded = decoder(z)\n",
        "vae = models.Model(inputs, decoded, name='vae_conv')\n",
        "\n",
        "# VAE loss: reconstruction + KL\n",
        "reconstruction_loss = tf.keras.losses.MeanSquaredError()(K.flatten(inputs), K.flatten(decoded))\n",
        "kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
        "vae_loss = reconstruction_loss + 1e-4 * kl_loss   # peso KLD pequeno, ajustar se necessário\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer=optimizers.Adam(1e-4))\n",
        "vae.summary()\n",
        "decoder.summary()\n",
        "\n",
        "# treinar como denoising: adiciona ruído leve nas entradas\n",
        "def add_noise(x, factor=NOISE_FACTOR):\n",
        "    noisy = x + factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape)\n",
        "    return np.clip(noisy, 0., 1.)\n",
        "\n",
        "es = callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
        "history_vae = vae.fit(\n",
        "    add_noise(train_X, NOISE_FACTOR), train_X,\n",
        "    epochs=VAE_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    validation_data=(add_noise(val_X, NOISE_FACTOR), val_X),\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "# salvar VAE/decoder\n",
        "vae.save(os.path.join(MODEL_DIR, \"vae_unit2.h5\"))\n",
        "decoder.save(os.path.join(MODEL_DIR, \"vae_decoder_unit2.h5\"))\n",
        "\n",
        "# salvar curvas de treino\n",
        "plt.figure()\n",
        "plt.plot(history_vae.history.get('loss', []), label='loss')\n",
        "plt.plot(history_vae.history.get('val_loss', []), label='val_loss')\n",
        "plt.legend(); plt.title(\"VAE loss\")\n",
        "plt.savefig(os.path.join(REPORT_DIR, \"vae_loss.png\"))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "XYSAlGgQy2b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 6 - Criar augmentações (novas abordagens)\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 6: criando augmentações (reconstruções VAE, interpolação latente, SpecAugment, spectral mix)\")\n",
        "\n",
        "ensure_empty_dir(AUG_DIR)\n",
        "for g in sorted(os.listdir(IMG_DIR)):\n",
        "    os.makedirs(os.path.join(AUG_DIR, g), exist_ok=True)\n",
        "\n",
        "# 6A: Reconstruções ruidosas (VAE reconstruction of noisy input)\n",
        "def reconstruct_with_vae(X, batch=32):\n",
        "    recs = vae.predict(X, batch_size=batch)\n",
        "    return recs\n",
        "\n",
        "# gerar reconstruções para cada imagem de treino (uma por amostra)\n",
        "for i, p in enumerate(tqdm(train_paths)):\n",
        "    cls = train_y[i]\n",
        "    # carregar imagem\n",
        "    img = Image.open(os.path.join(IMG_DIR, p)).convert(\"RGB\").resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "    arr = np.array(img, dtype=np.float32)/255.0\n",
        "    noisy = add_noise(np.expand_dims(arr,0), NOISE_FACTOR)\n",
        "    rec = vae.predict(noisy)[0]\n",
        "    outname = os.path.splitext(os.path.basename(p))[0] + \"_vae_rec.jpg\"\n",
        "    Image.fromarray((rec*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6B: Interpolação no espaço latente (entre pares da mesma classe e entre classes similares)\n",
        "N_INTERP_PER_CLASS = 2\n",
        "# Criar encoder model (input image -> z_mean) para obter médias latentes para interpolação\n",
        "encoder_model = models.Model(inputs, z_mean)  # usar z_mean (determinístico) para interpolar de forma estável\n",
        "\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    if len(class_indices) < 2: continue\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 10), replace=False)\n",
        "    for k in range(N_INTERP_PER_CLASS):\n",
        "        a,b = np.random.choice(chosen, size=2, replace=False)\n",
        "        img_a = np.array(Image.open(os.path.join(IMG_DIR, train_paths[a])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        img_b = np.array(Image.open(os.path.join(IMG_DIR, train_paths[b])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        za = encoder_model.predict(img_a[np.newaxis,...])[0]\n",
        "        zb = encoder_model.predict(img_b[np.newaxis,...])[0]\n",
        "        alpha = np.random.uniform(0.25, 0.75)\n",
        "        zint = alpha * za + (1-alpha) * zb\n",
        "        rec = decoder.predict(zint[np.newaxis,...])[0]\n",
        "        outname = f\"interp_{k}_{os.path.basename(train_paths[a])}_{os.path.basename(train_paths[b])}.jpg\"\n",
        "        Image.fromarray((np.clip(rec,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6C: SpecAugment (aplicado diretamente nos espectrogramas antes de salvar)\n",
        "def spec_augment_image(img_arr, time_masks=SPEC_AUG_TIME_MASKS, freq_masks=SPEC_AUG_FREQ_MASKS, max_time_mask_pct=0.2, max_freq_mask_pct=0.15):\n",
        "    # img_arr in [0,1], shape (H,W,C) but SpecAugment manipulates spectral axis (H=n_mels) and time axis (W)\n",
        "    img = img_arr.copy()\n",
        "    H, W = img.shape[0], img.shape[1]\n",
        "    for _ in range(time_masks):\n",
        "        t = int(np.random.uniform(0.0, max_time_mask_pct) * W)\n",
        "        t0 = np.random.randint(0, max(1, W - t + 1))\n",
        "        img[:, t0:t0+t, :] = 0\n",
        "    for _ in range(freq_masks):\n",
        "        f = int(np.random.uniform(0.0, max_freq_mask_pct) * H)\n",
        "        f0 = np.random.randint(0, max(1, H - f + 1))\n",
        "        img[f0:f0+f, :, :] = 0\n",
        "    return img\n",
        "\n",
        "# Aplicar SpecAugment a algumas imagens de treino e salvar\n",
        "N_SPEC_AUG_PER_CLASS = 2\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 20), replace=False)\n",
        "    for i_idx in chosen[:N_SPEC_AUG_PER_CLASS]:\n",
        "        p = train_paths[i_idx]\n",
        "        arr = np.array(Image.open(os.path.join(IMG_DIR, p)).convert(\"RGB\").resize((IMG_WIDTH, IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        aug = spec_augment_image(arr)\n",
        "        outname = os.path.splitext(os.path.basename(p))[0] + \"_specaug.jpg\"\n",
        "        Image.fromarray((np.clip(aug,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6D: Spectral Mix (linear mix of two spectrogram images) - naive mix (hard label use original or duplicate)\n",
        "N_MIX_PER_CLASS = 2\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    if len(class_indices) < 2: continue\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 20), replace=False)\n",
        "    for k in range(N_MIX_PER_CLASS):\n",
        "        a,b = np.random.choice(chosen, size=2, replace=False)\n",
        "        arr_a = np.array(Image.open(os.path.join(IMG_DIR, train_paths[a])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        arr_b = np.array(Image.open(os.path.join(IMG_DIR, train_paths[b])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        alpha = np.random.uniform(0.3, 0.7)\n",
        "        mixed = alpha*arr_a + (1-alpha)*arr_b\n",
        "        outname = f\"mix_{k}_{os.path.basename(train_paths[a])}_{os.path.basename(train_paths[b])}.jpg\"\n",
        "        Image.fromarray((np.clip(mixed,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6E: finalmente, copiamos os originais para AUG_DIR (mantém originais + artificially generated)\n",
        "for p, y in zip(train_paths, train_y):\n",
        "    src = os.path.join(IMG_DIR, p)\n",
        "    dst = os.path.join(AUG_DIR, y, os.path.basename(p))\n",
        "    if not os.path.exists(dst):\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "# salvar resumo de quantos arquivos por classe no AUG_DIR\n",
        "aug_counts = {g: len([f for f in os.listdir(os.path.join(AUG_DIR,g)) if f.lower().endswith(\".jpg\")]) for g in os.listdir(AUG_DIR)}\n",
        "pd.DataFrame(list(aug_counts.items()), columns=[\"genre\",\"aug_images\"]).to_csv(os.path.join(ROOT_DIR, \"aug_image_counts_unit2.csv\"), index=False)\n",
        "print(\"Augmentation concluída. Counts saved to aug_image_counts_unit2.csv\")"
      ],
      "metadata": {
        "id": "bWP5r5F1y7uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 7 - Testes: treinar 2 CNNs\n",
        "#   - CNN A: VGG16 transfer learning com dados originais (baseline)\n",
        "#   - CNN B: VGG16 transfer learning com dados originais + AUG_DIR\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 7: treinar CNNs baseline e augmented\")\n",
        "\n",
        "# Funções auxiliares para coletar arquivos\n",
        "def collect_files_labels(root):\n",
        "    files, labels = [], []\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        cls_dir = os.path.join(root, cls)\n",
        "        if not os.path.isdir(cls_dir): continue\n",
        "        for f in os.listdir(cls_dir):\n",
        "            if f.lower().endswith(\".jpg\"):\n",
        "                files.append(os.path.join(cls_dir, f))\n",
        "                labels.append(cls)\n",
        "    return files, labels\n",
        "\n",
        "orig_files, orig_labels = collect_files_labels(IMG_DIR)\n",
        "aug_files, aug_labels = collect_files_labels(AUG_DIR)\n",
        "\n",
        "# Criar splits (para comparação justa manter o mesmo test set - usar test_paths criados anteriormente)\n",
        "# We'll use test_paths list (relative paths) to define test set file paths in IMG_DIR\n",
        "test_file_paths = [os.path.join(IMG_DIR, p) for p in test_paths]\n",
        "\n",
        "# Create function to split train/val for baseline using orig_files excluding test_files\n",
        "def split_files_for_training(files, labels, test_file_paths, val_frac=0.2):\n",
        "    # filter out test\n",
        "    files_filtered, labels_filtered = [], []\n",
        "    for f, l in zip(files, labels):\n",
        "        if os.path.abspath(f) in [os.path.abspath(x) for x in test_file_paths]:\n",
        "            continue\n",
        "        files_filtered.append(f); labels_filtered.append(l)\n",
        "    tr_files, val_files, tr_labels, val_labels = train_test_split(files_filtered, labels_filtered, test_size=val_frac, stratify=labels_filtered, random_state=RANDOM_STATE)\n",
        "    return tr_files, val_files, tr_labels, val_labels\n",
        "\n",
        "train_files_base, val_files_base, train_labels_base, val_labels_base = split_files_for_training(orig_files, orig_labels, test_file_paths)\n",
        "train_files_aug, val_files_aug, train_labels_aug, val_labels_aug = split_files_for_training(aug_files, aug_labels, test_file_paths)\n",
        "\n",
        "print(\"Baseline train size:\", len(train_files_base), \"val:\", len(val_files_base), \"test:\", len(test_file_paths))\n",
        "print(\"Augmented train size:\", len(train_files_aug), \"val:\", len(val_files_aug), \"test:\", len(test_file_paths))\n",
        "\n",
        "# Generator Keras Sequence\n",
        "from tensorflow.keras.utils import Sequence\n",
        "class ImageSequence(Sequence):\n",
        "    def __init__(self, files, labels, le, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n",
        "        self.files = files\n",
        "        self.labels = np.array(labels)\n",
        "        self.le = le\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indexes = np.arange(len(self.files))\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.files)/self.batch_size)\n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_files = [self.files[i] for i in batch_idx]\n",
        "        batch_labels = self.labels[batch_idx]\n",
        "        imgs = []\n",
        "        for p in batch_files:\n",
        "            img = Image.open(p).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT))\n",
        "            arr = np.array(img, dtype=np.float32)/255.0\n",
        "            if self.augment:\n",
        "                # apply random small transforms: horizontal flip, small brightness jitter, spec augment occasionally\n",
        "                if random.random() < 0.1:\n",
        "                    arr = np.fliplr(arr)\n",
        "                if random.random() < 0.2:\n",
        "                    arr = np.clip(arr + 0.03 * np.random.randn(*arr.shape), 0, 1)\n",
        "            imgs.append(arr)\n",
        "        X = np.stack(imgs)\n",
        "        y = to_categorical(self.le.transform(batch_labels), num_classes=len(self.le.classes_))\n",
        "        return X, y\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "# label encoder based on classes found\n",
        "le_classes = LabelEncoder()\n",
        "le_classes.fit(sorted(os.listdir(IMG_DIR)))\n",
        "\n",
        "# Create generators\n",
        "train_gen_base = ImageSequence(train_files_base, train_labels_base, le_classes, batch_size=BATCH_SIZE, shuffle=True, augment=False)\n",
        "val_gen_base = ImageSequence(val_files_base, val_labels_base, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "train_gen_aug = ImageSequence(train_files_aug, train_labels_aug, le_classes, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
        "val_gen_aug = ImageSequence(val_files_aug, val_labels_aug, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "test_gen = ImageSequence(test_file_paths, test_y, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "\n",
        "# Build VGG16 transfer model factory\n",
        "def build_vgg_transfer(num_classes, train_base=False):\n",
        "    base = VGG16(include_top=False, weights='imagenet', input_shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "    base.trainable = train_base\n",
        "    inp = layers.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "num_classes = len(le_classes.classes_)\n",
        "print(\"Num classes:\", num_classes)\n",
        "\n",
        "# Train baseline\n",
        "print(\"Treinando VGG16 baseline (originais)...\")\n",
        "vgg_base = build_vgg_transfer(num_classes, train_base=False)\n",
        "es = callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
        "history_base = vgg_base.fit(train_gen_base, epochs=CLASS_EPOCHS, validation_data=val_gen_base, callbacks=[es])\n",
        "vgg_base.save(os.path.join(MODEL_DIR, \"vgg_base_unit2.h5\"))\n",
        "\n",
        "# Evaluate baseline\n",
        "def evaluate_keras_model(model, seq):\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "    for Xb, yb in seq:\n",
        "        probs = model.predict(Xb)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_prob.extend(probs.tolist())\n",
        "        y_true.extend(np.argmax(yb, axis=1).tolist())\n",
        "    return np.array(y_true), np.array(y_pred), np.array(y_prob)\n",
        "\n",
        "y_true_base, y_pred_base, y_prob_base = evaluate_keras_model(vgg_base, test_gen)\n",
        "acc_base = accuracy_score(y_true_base, y_pred_base)\n",
        "f1_base = f1_score(y_true_base, y_pred_base, average='macro')\n",
        "print(\"Baseline: Acc\", acc_base, \"F1\", f1_base)\n",
        "\n",
        "# Train augmented\n",
        "print(\"Treinando VGG16 com dataset aumentado...\")\n",
        "vgg_aug = build_vgg_transfer(num_classes, train_base=False)\n",
        "history_aug = vgg_aug.fit(train_gen_aug, epochs=CLASS_EPOCHS, validation_data=val_gen_aug, callbacks=[es])\n",
        "vgg_aug.save(os.path.join(MODEL_DIR, \"vgg_aug_unit2.h5\"))\n",
        "\n",
        "y_true_aug, y_pred_aug, y_prob_aug = evaluate_keras_model(vgg_aug, test_gen)\n",
        "acc_aug = accuracy_score(y_true_aug, y_pred_aug)\n",
        "f1_aug = f1_score(y_true_aug, y_pred_aug, average='macro')\n",
        "print(\"Augmented: Acc\", acc_aug, \"F1\", f1_aug)"
      ],
      "metadata": {
        "id": "XMUlI35sy_tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 8 - Resultados: tabela, ROC e matrizes de confusão\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 8: Gerando métricas, curvas ROC e matrizes de confusão (sem PDF).\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Criação da tabela de métricas\n",
        "y_test_bin = label_binarize(y_true_base, classes=np.arange(num_classes))\n",
        "try:\n",
        "    auc_base = roc_auc_score(y_test_bin, y_prob_base, average='macro')\n",
        "except Exception:\n",
        "    auc_base = np.nan\n",
        "try:\n",
        "    auc_aug = roc_auc_score(y_test_bin, y_prob_aug, average='macro')\n",
        "except Exception:\n",
        "    auc_aug = np.nan\n",
        "\n",
        "metrics_df = pd.DataFrame([\n",
        "    [\"VGG16 (baseline)\", acc_base, f1_base, auc_base],\n",
        "    [\"VGG16 (augmented)\", acc_aug, f1_aug, auc_aug]\n",
        "], columns=[\"Model\", \"Accuracy\", \"F1_macro\", \"AUC_macro\"])\n",
        "\n",
        "# Salvar tabela\n",
        "metrics_path = os.path.join(RESULTS_DIR, \"metrics_comparison_unit2.csv\")\n",
        "metrics_df.to_csv(metrics_path, index=False)\n",
        "print(\"\\n📊 Tabela de métricas:\")\n",
        "print(metrics_df)\n",
        "print(f\"\\nTabela salva em: {metrics_path}\")\n",
        "\n",
        "# --------------------------\n",
        "# Curvas ROC\n",
        "# --------------------------\n",
        "def multiclass_roc(y_true, y_prob, n_classes):\n",
        "    y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    for i in range(n_classes):\n",
        "        try:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], np.array(y_prob)[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        except:\n",
        "            fpr[i], tpr[i], roc_auc[i] = None, None, None\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes) if fpr[i] is not None]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        if fpr[i] is not None:\n",
        "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"], tpr[\"macro\"], roc_auc[\"macro\"] = all_fpr, mean_tpr, auc(all_fpr, mean_tpr)\n",
        "    return fpr, tpr, roc_auc\n",
        "\n",
        "fpr_b, tpr_b, roc_auc_b = multiclass_roc(y_true_base, y_prob_base, num_classes)\n",
        "fpr_a, tpr_a, roc_auc_a = multiclass_roc(y_true_aug, y_prob_aug, num_classes)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr_b[\"macro\"], tpr_b[\"macro\"], '--', label=f'Baseline (AUC={roc_auc_b[\"macro\"]:.3f})')\n",
        "plt.plot(fpr_a[\"macro\"], tpr_a[\"macro\"], '-.', label=f'Augmented (AUC={roc_auc_a[\"macro\"]:.3f})')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Comparação (Macro Average)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "roc_path = os.path.join(RESULTS_DIR, \"roc_comparison.png\")\n",
        "plt.savefig(roc_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"✅ Curva ROC salva em: {roc_path}\")\n",
        "\n",
        "# --------------------------\n",
        "# Matrizes de confusão\n",
        "# --------------------------\n",
        "cm_base = confusion_matrix(y_true_base, y_pred_base)\n",
        "cm_aug  = confusion_matrix(y_true_aug, y_pred_aug)\n",
        "\n",
        "# Baseline\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm_base, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=le_classes.classes_, yticklabels=le_classes.classes_)\n",
        "plt.title(\"Matriz de Confusão - CNN Base\")\n",
        "plt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\n",
        "cm_base_path = os.path.join(RESULTS_DIR, \"cm_baseline.png\")\n",
        "plt.savefig(cm_base_path, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# Augmented\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm_aug, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=le_classes.classes_, yticklabels=le_classes.classes_)\n",
        "plt.title(\"Matriz de Confusão - CNN com Augmentation\")\n",
        "plt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\n",
        "cm_aug_path = os.path.join(RESULTS_DIR, \"cm_augmented.png\")\n",
        "plt.savefig(cm_aug_path, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(f\"✅ Matrizes salvas em:\\n - {cm_base_path}\\n - {cm_aug_path}\")\n",
        "\n",
        "print(\"\\n🎯 Resultados finais concluídos! Verifique os arquivos na pasta 'results_unit2'\")\n"
      ],
      "metadata": {
        "id": "SOjhLBn401Pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}