{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yt_dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TelpVnVcZbg",
        "outputId": "754d1d69-4431-431b-e6e1-a58585cbd20d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2025.10.14-py3-none-any.whl.metadata (175 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.9/175.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.10.14-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp\n",
            "Successfully installed yt_dlp-2025.10.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5CfscjXKvl8l"
      },
      "outputs": [],
      "source": [
        "# --------------------------\n",
        "# 1 - installs, imports e diret√≥rios (ATUALIZADO)\n",
        "# --------------------------\n",
        "# se rodando em notebook: descomente a linha abaixo para garantir libs\n",
        "# !pip install numpy pandas matplotlib seaborn tqdm pillow librosa audioread yt-dlp ffmpeg-python scikit-learn xgboost tensorflow\n",
        "\n",
        "import os, re, shutil, subprocess, math, random, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa, librosa.display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K, callbacks, optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "\n",
        "# Fixar seeds\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --------------------------\n",
        "# Diret√≥rios principais (ajuste conforme seu Drive)\n",
        "# --------------------------\n",
        "\n",
        "ROOT_DIR = \"/content/drive/MyDrive/M.L_2UNIDADE\"   # caminho principal do projeto da 2¬™ unidade\n",
        "\n",
        "# Estrutura de pastas\n",
        "CSV_DIR     = os.path.join(ROOT_DIR, \"csv_files\")              # arquivos CSV do dataset\n",
        "COOKIES_DIR = os.path.join(ROOT_DIR, \"cookies\")                # cookies yt-dlp (opcional)\n",
        "WAV_DIR     = os.path.join(ROOT_DIR, \"wav_files\")              # cortes de √°udio WAV (10s)\n",
        "IMG_DIR     = os.path.join(ROOT_DIR, \"data-files\")             # espectrogramas originais\n",
        "AUG_DIR     = os.path.join(ROOT_DIR, \"data-files-augmented\")   # espectrogramas com augmentations\n",
        "MODEL_DIR   = os.path.join(ROOT_DIR, \"models_unit2\")           # modelos treinados (VAE, CNNs)\n",
        "RESULTS_DIR = os.path.join(ROOT_DIR, \"results_unit2\")          # m√©tricas, curvas ROC, matrizes de confus√£o\n",
        "\n",
        "# Criar diret√≥rios caso n√£o existam\n",
        "for d in [CSV_DIR, COOKIES_DIR, WAV_DIR, IMG_DIR, AUG_DIR, MODEL_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Caminhos espec√≠ficos\n",
        "CSV_PATH     = os.path.join(CSV_DIR, \"unbalanced_train_segments.csv\")  # caminho do CSV principal\n",
        "COOKIES_PATH = os.path.join(COOKIES_DIR, \"www.youtube.com_cookies.txt\")                 # cookies yt-dlp (se necess√°rio)\n",
        "\n",
        "# --------------------------\n",
        "# Par√¢metros principais\n",
        "# --------------------------\n",
        "TARGET_PER_CLASS = 250    # n√∫mero de m√∫sicas por g√™nero\n",
        "SR = 22050                # sample rate\n",
        "CLIP_DURATION = 10        # dura√ß√£o dos clipes em segundos\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "VAE_EPOCHS = 60\n",
        "CLASS_EPOCHS = 15\n",
        "LATENT_DIM = 64\n",
        "NOISE_FACTOR = 0.08\n",
        "SPEC_AUG_TIME_MASKS = 2\n",
        "SPEC_AUG_FREQ_MASKS = 2\n",
        "\n",
        "RANDOM_STATE = SEED\n",
        "\n",
        "# --------------------------\n",
        "# G√™neros alvo (iguais ao c√≥digo original; pode ajustar se quiser)\n",
        "# --------------------------\n",
        "genre_dict = {\n",
        "    '/m/064t9': 'Pop_music',\n",
        "    '/m/0glt670': 'Hip_hop_music',\n",
        "    '/m/06by7': 'Rock_music',\n",
        "    '/m/06j6l': 'Rhythm_blues',\n",
        "    '/m/06cqb': 'Reggae',\n",
        "    '/m/0y4f8': 'Vocal',\n",
        "    '/m/07gxw': 'Techno',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Helpers utilit√°rios\n",
        "# --------------------------\n",
        "def ensure_empty_dir(d):\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def plot_and_save_fig(fig, filename):\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(filename, dpi=200)\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "REzTtoBDyTop"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 2 - carregamento do dataset (leitura do CSV e filtragem por g√™neros)\n",
        "# --------------------------\n",
        "print(\"Etapa 2: lendo CSV e preparando candidatos...\")\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"CSV n√£o encontrado em {CSV_PATH} - verifique caminho\")\n",
        "\n",
        "data = []\n",
        "with open(CSV_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "        elements = re.sub(r'[\"\\n]', \"\", line).split(\",\")\n",
        "        if len(elements) >= 4:\n",
        "            url = elements[0]; start = elements[1]; end = elements[2]; labels = elements[3:]\n",
        "            for label in labels:\n",
        "                if label in genre_dict:\n",
        "                    data.append([url, start, end, genre_dict[label]])\n",
        "df = pd.DataFrame(data, columns=[\"url\", \"start_time\", \"end_time\", \"class_label\"])\n",
        "df.to_csv(os.path.join(ROOT_DIR, \"df_candidates_unit2.csv\"), index=False)\n",
        "print(\"Total candidatos lidos:\", len(df))\n",
        "print(df[\"class_label\"].value_counts())"
      ],
      "metadata": {
        "id": "MN4zR-23yXin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23f1077-7cc0-4b6d-d820-6e17fb7b1272"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etapa 2: lendo CSV e preparando candidatos...\n",
            "Total candidatos lidos: 51710\n",
            "class_label\n",
            "Techno           16811\n",
            "Pop_music         8407\n",
            "Rock_music        8198\n",
            "Hip_hop_music     7370\n",
            "Rhythm_blues      4755\n",
            "Vocal             3241\n",
            "Reggae            2928\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 3 - corte dos v√≠deos (download e cut) ‚Äî com retomada ordenada\n",
        "# --------------------------\n",
        "COOKIES_PATH = os.path.join(COOKIES_DIR, \"www.youtube.com_cookies.txt\")\n",
        "print(\"\\nEtapa 3: Download e corte dos v√≠deos (com retomada ordenada e verifica√ß√£o incremental).\")\n",
        "\n",
        "import yt_dlp\n",
        "import pandas as pd\n",
        "import os, subprocess\n",
        "\n",
        "# üî∏ N√ÉO limpar o diret√≥rio\n",
        "os.makedirs(WAV_DIR, exist_ok=True)\n",
        "\n",
        "# üî∏ Verificar progresso anterior\n",
        "progress_file = os.path.join(ROOT_DIR, \"df_success_unit2.csv\")\n",
        "if os.path.exists(progress_file):\n",
        "    df_success = pd.read_csv(progress_file)\n",
        "    downloaded_ids = set(df_success[\"url\"].astype(str))\n",
        "    print(f\"‚úÖ Retomando: {len(downloaded_ids)} v√≠deos j√° baixados com sucesso.\")\n",
        "else:\n",
        "    df_success = pd.DataFrame(columns=[\"url\", \"start_time\", \"end_time\", \"class_label\"])\n",
        "    downloaded_ids = set()\n",
        "\n",
        "# üî∏ Contar quantos arquivos WAV j√° existem por g√™nero\n",
        "existing_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "existing_counts = {}\n",
        "for f in existing_files:\n",
        "    try:\n",
        "        genre_name = f.split(\"_\", 1)[1][:-4]\n",
        "        existing_counts[genre_name] = existing_counts.get(genre_name, 0) + 1\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "counts = {label: existing_counts.get(label, 0) for label in genre_dict.values()}\n",
        "print(\"üìä Arquivos j√° existentes por g√™nero:\", counts)\n",
        "\n",
        "# üî∏ Determinar o √∫ltimo v√≠deo baixado com sucesso (para cada g√™nero)\n",
        "last_positions = {}\n",
        "if not df_success.empty:\n",
        "    for genre in df_success[\"class_label\"].unique():\n",
        "        last_url = df_success[df_success[\"class_label\"] == genre][\"url\"].iloc[-1]\n",
        "        try:\n",
        "            pos = df[df[\"url\"] == last_url].index[-1]\n",
        "            last_positions[genre] = pos\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "# üî∏ Fun√ß√£o auxiliar: verificar se j√° existe WAV\n",
        "def wav_exists(video_id, label):\n",
        "    pattern = f\"{video_id}_{label}.wav\"\n",
        "    return any(pattern in f for f in existing_files)\n",
        "\n",
        "# üî∏ Baixar somente os g√™neros incompletos, continuando do √∫ltimo baixado\n",
        "downloaded_rows = []\n",
        "for label, group in df.groupby(\"class_label\"):\n",
        "    current_count = counts.get(label, 0)\n",
        "    if current_count >= TARGET_PER_CLASS:\n",
        "        print(f\"‚úÖ {label} j√° completo ({current_count}/{TARGET_PER_CLASS}) - pulando.\")\n",
        "        continue\n",
        "\n",
        "    # se houver posi√ß√£o salva, come√ßa dali\n",
        "    start_index = last_positions.get(label, group.index[0])\n",
        "    group = group.loc[group.index >= start_index]\n",
        "\n",
        "    needed = TARGET_PER_CLASS - current_count\n",
        "    print(f\"\\nüîπ {label}: precisa baixar {needed} arquivos (continuando ap√≥s √≠ndice {start_index}).\")\n",
        "\n",
        "    for idx, row in group.iterrows():\n",
        "        vid = str(row[\"url\"])\n",
        "        start = float(row[\"start_time\"])\n",
        "        url = f\"https://www.youtube.com/watch?v={vid}\"\n",
        "        final_file = os.path.join(WAV_DIR, f\"{vid}_{label}.wav\")\n",
        "\n",
        "        # j√° baixado? pula\n",
        "        if vid in downloaded_ids or wav_exists(vid, label):\n",
        "            continue\n",
        "        if counts[label] >= TARGET_PER_CLASS:\n",
        "            print(f\"üéØ {label} atingiu {TARGET_PER_CLASS}, parando.\")\n",
        "            break\n",
        "\n",
        "        temp_out = os.path.join(WAV_DIR, f\"temp_{vid}.%(ext)s\")\n",
        "        try:\n",
        "            ydl_opts = {\n",
        "                \"format\": \"bestaudio/best\",\n",
        "                \"outtmpl\": temp_out,\n",
        "                \"quiet\": True,\n",
        "                \"noplaylist\": True\n",
        "            }\n",
        "            if os.path.exists(COOKIES_PATH):\n",
        "                ydl_opts[\"cookiefile\"] = COOKIES_PATH\n",
        "\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([url])\n",
        "\n",
        "            temp_files = [f for f in os.listdir(WAV_DIR) if f.startswith(f\"temp_{vid}\")]\n",
        "            if not temp_files:\n",
        "                print(f\"‚ö†Ô∏è Nenhum arquivo tempor√°rio gerado para {vid}\")\n",
        "                continue\n",
        "\n",
        "            temp_file = os.path.join(WAV_DIR, temp_files[0])\n",
        "            subprocess.run([\n",
        "                \"ffmpeg\", \"-y\", \"-ss\", str(start), \"-t\", str(CLIP_DURATION),\n",
        "                \"-i\", temp_file, \"-acodec\", \"pcm_s16le\", \"-ar\", str(SR), \"-ac\", \"1\", final_file\n",
        "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            os.remove(temp_file)\n",
        "            if os.path.exists(final_file):\n",
        "                counts[label] += 1\n",
        "                downloaded_rows.append(row)\n",
        "                downloaded_ids.add(vid)\n",
        "\n",
        "                # salvar progresso incremental\n",
        "                pd.concat([df_success, pd.DataFrame([row])]).to_csv(progress_file, index=False)\n",
        "\n",
        "                if counts[label] % 10 == 0:\n",
        "                    print(f\"üéµ {counts[label]} baixados para {label}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è WAV n√£o gerado para {vid}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao baixar {vid}: {e}\")\n",
        "            continue\n",
        "\n",
        "# üî∏ Salvar resumo\n",
        "summary = pd.DataFrame(list(counts.items()), columns=[\"genre\", \"count\"])\n",
        "summary.to_csv(os.path.join(ROOT_DIR, \"dataset_summary_unit2.csv\"), index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Download finalizado.\")\n",
        "for g, c in counts.items():\n",
        "    print(f\" - {g}: {c}/{TARGET_PER_CLASS}\")\n",
        "print(\"Progresso salvo em dataset_summary_unit2.csv e df_success_unit2.csv\")\n"
      ],
      "metadata": {
        "id": "VJ2sofWSyawI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290f457f-0b72-46b6-be9d-2e123d5e3125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etapa 3: Download e corte dos v√≠deos (com retomada ordenada e verifica√ß√£o incremental).\n",
            "‚úÖ Retomando: 10 v√≠deos j√° baixados com sucesso.\n",
            "üìä Arquivos j√° existentes por g√™nero: {'Pop_music': 250, 'Hip_hop_music': 250, 'Rock_music': 249, 'Rhythm_blues': 250, 'Reggae': 250, 'Vocal': 145, 'Techno': 250}\n",
            "‚úÖ Hip_hop_music j√° completo (250/250) - pulando.\n",
            "‚úÖ Pop_music j√° completo (250/250) - pulando.\n",
            "‚úÖ Reggae j√° completo (250/250) - pulando.\n",
            "‚úÖ Rhythm_blues j√° completo (250/250) - pulando.\n",
            "\n",
            "üîπ Rock_music: precisa baixar 1 arquivos (continuando ap√≥s √≠ndice 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -08IuAXloCI: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -08IuAXloCI: ERROR: [youtube] -08IuAXloCI: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -5HSR5eWEDU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -5HSR5eWEDU: ERROR: [youtube] -5HSR5eWEDU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -82OBRkQskQ: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -82OBRkQskQ: ERROR: [youtube] -82OBRkQskQ: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -LTES9d6dZY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -LTES9d6dZY: ERROR: [youtube] -LTES9d6dZY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -TdKFIt-tlY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -TdKFIt-tlY: ERROR: [youtube] -TdKFIt-tlY: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -XCNxDxsAL0: Video unavailable. This video is no longer available due to a copyright claim by Rock & Roll Hall of Fame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -XCNxDxsAL0: ERROR: [youtube] -XCNxDxsAL0: Video unavailable. This video is no longer available due to a copyright claim by Rock & Roll Hall of Fame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -aA0j_NIJcg: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -aA0j_NIJcg: ERROR: [youtube] -aA0j_NIJcg: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -fTW98nVe1g: Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -fTW98nVe1g: ERROR: [youtube] -fTW98nVe1g: Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -hrpU1nxsbc: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -hrpU1nxsbc: ERROR: [youtube] -hrpU1nxsbc: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -lAVvperqpQ: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -lAVvperqpQ: ERROR: [youtube] -lAVvperqpQ: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -pRIE4KkY1M: Video unavailable. This video is no longer available due to a copyright claim by Coda Publishing Ltd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar -pRIE4KkY1M: ERROR: [youtube] -pRIE4KkY1M: Video unavailable. This video is no longer available due to a copyright claim by Coda Publishing Ltd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 061qiXuzrQs: Video unavailable. This video is no longer available due to a copyright claim by Studio Hamburg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 061qiXuzrQs: ERROR: [youtube] 061qiXuzrQs: Video unavailable. This video is no longer available due to a copyright claim by Studio Hamburg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0AQvRSZ8ZNo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0AQvRSZ8ZNo: ERROR: [youtube] 0AQvRSZ8ZNo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0H3VllGZQEU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0H3VllGZQEU: ERROR: [youtube] 0H3VllGZQEU: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0Ma-YlsySuU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0Ma-YlsySuU: ERROR: [youtube] 0Ma-YlsySuU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0YRM3tkIXfA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0YRM3tkIXfA: ERROR: [youtube] 0YRM3tkIXfA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0q_EX7TiuV8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0q_EX7TiuV8: ERROR: [youtube] 0q_EX7TiuV8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0sMqbNNwzvU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0sMqbNNwzvU: ERROR: [youtube] 0sMqbNNwzvU: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0sqcvMHyQ1k: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0sqcvMHyQ1k: ERROR: [youtube] 0sqcvMHyQ1k: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 0z7EpFwegJM: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 0z7EpFwegJM: ERROR: [youtube] 0z7EpFwegJM: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 1-Oahk8L_Og: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 1-Oahk8L_Og: ERROR: [youtube] 1-Oahk8L_Og: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] 1A8piNIeg6w: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar 1A8piNIeg6w: ERROR: [youtube] 1A8piNIeg6w: Video unavailable\n",
            "üéµ 250 baixados para Rock_music\n",
            "üéØ Rock_music atingiu 250, parando.\n",
            "‚úÖ Techno j√° completo (250/250) - pulando.\n",
            "\n",
            "üîπ Vocal: precisa baixar 105 arquivos (continuando ap√≥s √≠ndice 44967).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nI59vkjjZY0: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar nI59vkjjZY0: ERROR: [youtube] nI59vkjjZY0: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nIA8YdQGM1Y: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar nIA8YdQGM1Y: ERROR: [youtube] nIA8YdQGM1Y: Video unavailable. This video is private\n",
            "üéµ 150 baixados para Vocal\n",
            "üéµ 160 baixados para Vocal\n",
            "üéµ 170 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nf7AArmXhys: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar nf7AArmXhys: ERROR: [youtube] nf7AArmXhys: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] npUieylloJE: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar npUieylloJE: ERROR: [youtube] npUieylloJE: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nqO5FTTuCvA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar nqO5FTTuCvA: ERROR: [youtube] nqO5FTTuCvA: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
            "üéµ 180 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] nzldgEJxVds: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar nzldgEJxVds: ERROR: [youtube] nzldgEJxVds: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] o8Kc4i9YuZ4: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar o8Kc4i9YuZ4: ERROR: [youtube] o8Kc4i9YuZ4: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
            "üéµ 190 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] oIyj9aDz74A: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar oIyj9aDz74A: ERROR: [youtube] oIyj9aDz74A: Video unavailable. This video is private\n",
            "üéµ 200 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] oiWZWUQx_nY: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar oiWZWUQx_nY: ERROR: [youtube] oiWZWUQx_nY: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] ojVK0Tf2Udk: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar ojVK0Tf2Udk: ERROR: [youtube] ojVK0Tf2Udk: Video unavailable\n",
            "üéµ 210 baixados para Vocal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] olxNrhVurkI: Video unavailable. This video is private\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Erro ao baixar olxNrhVurkI: ERROR: [youtube] olxNrhVurkI: Video unavailable. This video is private\n",
            "üéµ 220 baixados para Vocal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4 - gerar os espectrogramas (MEL)\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 4: gerando espectrogramas MEL (224x224)...\")\n",
        "ensure_empty_dir(IMG_DIR)\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "wav_files = [f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")]\n",
        "for f in tqdm(wav_files):\n",
        "    try:\n",
        "        class_name = f.split(\"_\",1)[1][:-4]\n",
        "        class_dir = os.path.join(IMG_DIR, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        y, sr = librosa.load(os.path.join(WAV_DIR, f), sr=SR)\n",
        "        # pre-emphasis\n",
        "        y = np.append(y[0], y[1:] - 0.97*y[:-1])\n",
        "        M = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128, fmax=sr/2)\n",
        "        log_power = librosa.power_to_db(M, ref=np.max)\n",
        "        # plot\n",
        "        plt.figure(figsize=(3,3))\n",
        "        plt.axis('off')\n",
        "        librosa.display.specshow(log_power, sr=sr, hop_length=512, cmap='jet')\n",
        "        savepath = os.path.join(class_dir, f[:-4] + \".jpg\")\n",
        "        plt.savefig(savepath, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao processar\", f, e)\n",
        "\n",
        "# contar imagens por genero e salvar\n",
        "counts_img = {}\n",
        "for g in os.listdir(IMG_DIR):\n",
        "    gp = os.path.join(IMG_DIR, g)\n",
        "    if os.path.isdir(gp):\n",
        "        counts_img[g] = len([x for x in os.listdir(gp) if x.lower().endswith(\".jpg\")])\n",
        "pd.DataFrame(list(counts_img.items()), columns=[\"genre\",\"images\"]).to_csv(os.path.join(ROOT_DIR, \"image_counts_unit2.csv\"), index=False)\n",
        "print(\"Contagem de espectrogramas salva em image_counts_unit2.csv\")"
      ],
      "metadata": {
        "id": "03SuC-p7yueR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 5 - criar e treinar o autoencoder (VAE conv) - novidade\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 5: construindo e treinando VAE conv (denoising VAE).\")\n",
        "\n",
        "# carregando imagens de treino (tudo por enquanto, separaremos depois)\n",
        "def load_images_by_class(img_root, resize=(IMG_WIDTH, IMG_HEIGHT), max_per_class=None):\n",
        "    X, y, paths = [], [], []\n",
        "    for cls in sorted(os.listdir(img_root)):\n",
        "        cls_dir = os.path.join(img_root, cls)\n",
        "        if not os.path.isdir(cls_dir): continue\n",
        "        files = sorted([f for f in os.listdir(cls_dir) if f.lower().endswith(\".jpg\")])\n",
        "        if max_per_class:\n",
        "            files = files[:max_per_class]\n",
        "        for f in files:\n",
        "            img = Image.open(os.path.join(cls_dir,f)).convert(\"RGB\").resize(resize)\n",
        "            arr = np.array(img, dtype=np.float32)/255.0\n",
        "            X.append(arr)\n",
        "            y.append(cls)\n",
        "            paths.append(os.path.join(cls, f))\n",
        "    return np.stack(X), np.array(y), paths\n",
        "\n",
        "# Carrega tudo (aten√ß√£o mem√≥ria)\n",
        "X_all, y_all, paths_all = load_images_by_class(IMG_DIR, max_per_class=None)\n",
        "print(\"Total imagens espectrograma:\", X_all.shape)\n",
        "\n",
        "# separar treino/val/test (estratificado por genero)\n",
        "train_X, test_X, train_y, test_y, train_paths, test_paths = train_test_split(\n",
        "    X_all, y_all, paths_all, test_size=0.2, stratify=y_all, random_state=RANDOM_STATE)\n",
        "train_X, val_X, train_y, val_y, train_paths, val_paths = train_test_split(\n",
        "    train_X, train_y, train_paths, test_size=0.2, stratify=train_y, random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"Splits -> train:\", train_X.shape[0], \"val:\", val_X.shape[0], \"test:\", test_X.shape[0])\n",
        "\n",
        "# VAE model (encoder + sampling + decoder)\n",
        "input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "# encoder\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "x = layers.MaxPooling2D(2, padding='same')(x)           # 112x112\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D(2, padding='same')(x)           # 56x56\n",
        "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D(2, padding='same')(x)           # 28x28\n",
        "shape_before_flatten = K.int_shape(x)[1:]               # (28,28,128)\n",
        "flat = layers.Flatten()(x)\n",
        "h = layers.Dense(512, activation='relu')(flat)\n",
        "\n",
        "z_mean = layers.Dense(LATENT_DIM, name='z_mean')(h)\n",
        "z_log_var = layers.Dense(LATENT_DIM, name='z_log_var')(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], LATENT_DIM), mean=0., stddev=1.0)\n",
        "    return z_mean + K.exp(0.5*z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling, output_shape=(LATENT_DIM,))([z_mean, z_log_var])\n",
        "\n",
        "# decoder\n",
        "decoder_input = layers.Input(shape=(LATENT_DIM,))\n",
        "d = layers.Dense(np.prod(shape_before_flatten), activation='relu')(decoder_input)\n",
        "d = layers.Reshape(shape_before_flatten)(d)\n",
        "d = layers.Conv2D(128, 3, activation='relu', padding='same')(d)\n",
        "d = layers.UpSampling2D(2)(d)  # 56\n",
        "d = layers.Conv2D(64, 3, activation='relu', padding='same')(d)\n",
        "d = layers.UpSampling2D(2)(d)  # 112\n",
        "d = layers.Conv2D(32, 3, activation='relu', padding='same')(d)\n",
        "d = layers.UpSampling2D(2)(d)  # 224\n",
        "d = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(d)\n",
        "decoder = models.Model(decoder_input, d, name='decoder_model')\n",
        "\n",
        "# full VAE model\n",
        "decoded = decoder(z)\n",
        "vae = models.Model(inputs, decoded, name='vae_conv')\n",
        "\n",
        "# VAE loss: reconstruction + KL\n",
        "reconstruction_loss = tf.keras.losses.MeanSquaredError()(K.flatten(inputs), K.flatten(decoded))\n",
        "kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
        "vae_loss = reconstruction_loss + 1e-4 * kl_loss   # peso KLD pequeno, ajustar se necess√°rio\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer=optimizers.Adam(1e-4))\n",
        "vae.summary()\n",
        "decoder.summary()\n",
        "\n",
        "# treinar como denoising: adiciona ru√≠do leve nas entradas\n",
        "def add_noise(x, factor=NOISE_FACTOR):\n",
        "    noisy = x + factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape)\n",
        "    return np.clip(noisy, 0., 1.)\n",
        "\n",
        "es = callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
        "history_vae = vae.fit(\n",
        "    add_noise(train_X, NOISE_FACTOR), train_X,\n",
        "    epochs=VAE_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    validation_data=(add_noise(val_X, NOISE_FACTOR), val_X),\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "# salvar VAE/decoder\n",
        "vae.save(os.path.join(MODEL_DIR, \"vae_unit2.h5\"))\n",
        "decoder.save(os.path.join(MODEL_DIR, \"vae_decoder_unit2.h5\"))\n",
        "\n",
        "# salvar curvas de treino\n",
        "plt.figure()\n",
        "plt.plot(history_vae.history.get('loss', []), label='loss')\n",
        "plt.plot(history_vae.history.get('val_loss', []), label='val_loss')\n",
        "plt.legend(); plt.title(\"VAE loss\")\n",
        "plt.savefig(os.path.join(REPORT_DIR, \"vae_loss.png\"))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "XYSAlGgQy2b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 6 - Criar augmenta√ß√µes (novas abordagens)\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 6: criando augmenta√ß√µes (reconstru√ß√µes VAE, interpola√ß√£o latente, SpecAugment, spectral mix)\")\n",
        "\n",
        "ensure_empty_dir(AUG_DIR)\n",
        "for g in sorted(os.listdir(IMG_DIR)):\n",
        "    os.makedirs(os.path.join(AUG_DIR, g), exist_ok=True)\n",
        "\n",
        "# 6A: Reconstru√ß√µes ruidosas (VAE reconstruction of noisy input)\n",
        "def reconstruct_with_vae(X, batch=32):\n",
        "    recs = vae.predict(X, batch_size=batch)\n",
        "    return recs\n",
        "\n",
        "# gerar reconstru√ß√µes para cada imagem de treino (uma por amostra)\n",
        "for i, p in enumerate(tqdm(train_paths)):\n",
        "    cls = train_y[i]\n",
        "    # carregar imagem\n",
        "    img = Image.open(os.path.join(IMG_DIR, p)).convert(\"RGB\").resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "    arr = np.array(img, dtype=np.float32)/255.0\n",
        "    noisy = add_noise(np.expand_dims(arr,0), NOISE_FACTOR)\n",
        "    rec = vae.predict(noisy)[0]\n",
        "    outname = os.path.splitext(os.path.basename(p))[0] + \"_vae_rec.jpg\"\n",
        "    Image.fromarray((rec*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6B: Interpola√ß√£o no espa√ßo latente (entre pares da mesma classe e entre classes similares)\n",
        "N_INTERP_PER_CLASS = 2\n",
        "# Criar encoder model (input image -> z_mean) para obter m√©dias latentes para interpola√ß√£o\n",
        "encoder_model = models.Model(inputs, z_mean)  # usar z_mean (determin√≠stico) para interpolar de forma est√°vel\n",
        "\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    if len(class_indices) < 2: continue\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 10), replace=False)\n",
        "    for k in range(N_INTERP_PER_CLASS):\n",
        "        a,b = np.random.choice(chosen, size=2, replace=False)\n",
        "        img_a = np.array(Image.open(os.path.join(IMG_DIR, train_paths[a])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        img_b = np.array(Image.open(os.path.join(IMG_DIR, train_paths[b])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        za = encoder_model.predict(img_a[np.newaxis,...])[0]\n",
        "        zb = encoder_model.predict(img_b[np.newaxis,...])[0]\n",
        "        alpha = np.random.uniform(0.25, 0.75)\n",
        "        zint = alpha * za + (1-alpha) * zb\n",
        "        rec = decoder.predict(zint[np.newaxis,...])[0]\n",
        "        outname = f\"interp_{k}_{os.path.basename(train_paths[a])}_{os.path.basename(train_paths[b])}.jpg\"\n",
        "        Image.fromarray((np.clip(rec,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6C: SpecAugment (aplicado diretamente nos espectrogramas antes de salvar)\n",
        "def spec_augment_image(img_arr, time_masks=SPEC_AUG_TIME_MASKS, freq_masks=SPEC_AUG_FREQ_MASKS, max_time_mask_pct=0.2, max_freq_mask_pct=0.15):\n",
        "    # img_arr in [0,1], shape (H,W,C) but SpecAugment manipulates spectral axis (H=n_mels) and time axis (W)\n",
        "    img = img_arr.copy()\n",
        "    H, W = img.shape[0], img.shape[1]\n",
        "    for _ in range(time_masks):\n",
        "        t = int(np.random.uniform(0.0, max_time_mask_pct) * W)\n",
        "        t0 = np.random.randint(0, max(1, W - t + 1))\n",
        "        img[:, t0:t0+t, :] = 0\n",
        "    for _ in range(freq_masks):\n",
        "        f = int(np.random.uniform(0.0, max_freq_mask_pct) * H)\n",
        "        f0 = np.random.randint(0, max(1, H - f + 1))\n",
        "        img[f0:f0+f, :, :] = 0\n",
        "    return img\n",
        "\n",
        "# Aplicar SpecAugment a algumas imagens de treino e salvar\n",
        "N_SPEC_AUG_PER_CLASS = 2\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 20), replace=False)\n",
        "    for i_idx in chosen[:N_SPEC_AUG_PER_CLASS]:\n",
        "        p = train_paths[i_idx]\n",
        "        arr = np.array(Image.open(os.path.join(IMG_DIR, p)).convert(\"RGB\").resize((IMG_WIDTH, IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        aug = spec_augment_image(arr)\n",
        "        outname = os.path.splitext(os.path.basename(p))[0] + \"_specaug.jpg\"\n",
        "        Image.fromarray((np.clip(aug,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6D: Spectral Mix (linear mix of two spectrogram images) - naive mix (hard label use original or duplicate)\n",
        "N_MIX_PER_CLASS = 2\n",
        "for cls in sorted(set(train_y)):\n",
        "    class_indices = [i for i,lab in enumerate(train_y) if lab==cls]\n",
        "    if len(class_indices) < 2: continue\n",
        "    chosen = np.random.choice(class_indices, size=min(len(class_indices), 20), replace=False)\n",
        "    for k in range(N_MIX_PER_CLASS):\n",
        "        a,b = np.random.choice(chosen, size=2, replace=False)\n",
        "        arr_a = np.array(Image.open(os.path.join(IMG_DIR, train_paths[a])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        arr_b = np.array(Image.open(os.path.join(IMG_DIR, train_paths[b])).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT)), dtype=np.float32)/255.0\n",
        "        alpha = np.random.uniform(0.3, 0.7)\n",
        "        mixed = alpha*arr_a + (1-alpha)*arr_b\n",
        "        outname = f\"mix_{k}_{os.path.basename(train_paths[a])}_{os.path.basename(train_paths[b])}.jpg\"\n",
        "        Image.fromarray((np.clip(mixed,0,1)*255).astype(np.uint8)).save(os.path.join(AUG_DIR, cls, outname))\n",
        "\n",
        "# 6E: finalmente, copiamos os originais para AUG_DIR (mant√©m originais + artificially generated)\n",
        "for p, y in zip(train_paths, train_y):\n",
        "    src = os.path.join(IMG_DIR, p)\n",
        "    dst = os.path.join(AUG_DIR, y, os.path.basename(p))\n",
        "    if not os.path.exists(dst):\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "# salvar resumo de quantos arquivos por classe no AUG_DIR\n",
        "aug_counts = {g: len([f for f in os.listdir(os.path.join(AUG_DIR,g)) if f.lower().endswith(\".jpg\")]) for g in os.listdir(AUG_DIR)}\n",
        "pd.DataFrame(list(aug_counts.items()), columns=[\"genre\",\"aug_images\"]).to_csv(os.path.join(ROOT_DIR, \"aug_image_counts_unit2.csv\"), index=False)\n",
        "print(\"Augmentation conclu√≠da. Counts saved to aug_image_counts_unit2.csv\")"
      ],
      "metadata": {
        "id": "bWP5r5F1y7uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 7 - Testes: treinar 2 CNNs\n",
        "#   - CNN A: VGG16 transfer learning com dados originais (baseline)\n",
        "#   - CNN B: VGG16 transfer learning com dados originais + AUG_DIR\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 7: treinar CNNs baseline e augmented\")\n",
        "\n",
        "# Fun√ß√µes auxiliares para coletar arquivos\n",
        "def collect_files_labels(root):\n",
        "    files, labels = [], []\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        cls_dir = os.path.join(root, cls)\n",
        "        if not os.path.isdir(cls_dir): continue\n",
        "        for f in os.listdir(cls_dir):\n",
        "            if f.lower().endswith(\".jpg\"):\n",
        "                files.append(os.path.join(cls_dir, f))\n",
        "                labels.append(cls)\n",
        "    return files, labels\n",
        "\n",
        "orig_files, orig_labels = collect_files_labels(IMG_DIR)\n",
        "aug_files, aug_labels = collect_files_labels(AUG_DIR)\n",
        "\n",
        "# Criar splits (para compara√ß√£o justa manter o mesmo test set - usar test_paths criados anteriormente)\n",
        "# We'll use test_paths list (relative paths) to define test set file paths in IMG_DIR\n",
        "test_file_paths = [os.path.join(IMG_DIR, p) for p in test_paths]\n",
        "\n",
        "# Create function to split train/val for baseline using orig_files excluding test_files\n",
        "def split_files_for_training(files, labels, test_file_paths, val_frac=0.2):\n",
        "    # filter out test\n",
        "    files_filtered, labels_filtered = [], []\n",
        "    for f, l in zip(files, labels):\n",
        "        if os.path.abspath(f) in [os.path.abspath(x) for x in test_file_paths]:\n",
        "            continue\n",
        "        files_filtered.append(f); labels_filtered.append(l)\n",
        "    tr_files, val_files, tr_labels, val_labels = train_test_split(files_filtered, labels_filtered, test_size=val_frac, stratify=labels_filtered, random_state=RANDOM_STATE)\n",
        "    return tr_files, val_files, tr_labels, val_labels\n",
        "\n",
        "train_files_base, val_files_base, train_labels_base, val_labels_base = split_files_for_training(orig_files, orig_labels, test_file_paths)\n",
        "train_files_aug, val_files_aug, train_labels_aug, val_labels_aug = split_files_for_training(aug_files, aug_labels, test_file_paths)\n",
        "\n",
        "print(\"Baseline train size:\", len(train_files_base), \"val:\", len(val_files_base), \"test:\", len(test_file_paths))\n",
        "print(\"Augmented train size:\", len(train_files_aug), \"val:\", len(val_files_aug), \"test:\", len(test_file_paths))\n",
        "\n",
        "# Generator Keras Sequence\n",
        "from tensorflow.keras.utils import Sequence\n",
        "class ImageSequence(Sequence):\n",
        "    def __init__(self, files, labels, le, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n",
        "        self.files = files\n",
        "        self.labels = np.array(labels)\n",
        "        self.le = le\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indexes = np.arange(len(self.files))\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.files)/self.batch_size)\n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_files = [self.files[i] for i in batch_idx]\n",
        "        batch_labels = self.labels[batch_idx]\n",
        "        imgs = []\n",
        "        for p in batch_files:\n",
        "            img = Image.open(p).convert(\"RGB\").resize((IMG_WIDTH,IMG_HEIGHT))\n",
        "            arr = np.array(img, dtype=np.float32)/255.0\n",
        "            if self.augment:\n",
        "                # apply random small transforms: horizontal flip, small brightness jitter, spec augment occasionally\n",
        "                if random.random() < 0.1:\n",
        "                    arr = np.fliplr(arr)\n",
        "                if random.random() < 0.2:\n",
        "                    arr = np.clip(arr + 0.03 * np.random.randn(*arr.shape), 0, 1)\n",
        "            imgs.append(arr)\n",
        "        X = np.stack(imgs)\n",
        "        y = to_categorical(self.le.transform(batch_labels), num_classes=len(self.le.classes_))\n",
        "        return X, y\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "# label encoder based on classes found\n",
        "le_classes = LabelEncoder()\n",
        "le_classes.fit(sorted(os.listdir(IMG_DIR)))\n",
        "\n",
        "# Create generators\n",
        "train_gen_base = ImageSequence(train_files_base, train_labels_base, le_classes, batch_size=BATCH_SIZE, shuffle=True, augment=False)\n",
        "val_gen_base = ImageSequence(val_files_base, val_labels_base, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "train_gen_aug = ImageSequence(train_files_aug, train_labels_aug, le_classes, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
        "val_gen_aug = ImageSequence(val_files_aug, val_labels_aug, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "test_gen = ImageSequence(test_file_paths, test_y, le_classes, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "\n",
        "# Build VGG16 transfer model factory\n",
        "def build_vgg_transfer(num_classes, train_base=False):\n",
        "    base = VGG16(include_top=False, weights='imagenet', input_shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "    base.trainable = train_base\n",
        "    inp = layers.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "num_classes = len(le_classes.classes_)\n",
        "print(\"Num classes:\", num_classes)\n",
        "\n",
        "# Train baseline\n",
        "print(\"Treinando VGG16 baseline (originais)...\")\n",
        "vgg_base = build_vgg_transfer(num_classes, train_base=False)\n",
        "es = callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
        "history_base = vgg_base.fit(train_gen_base, epochs=CLASS_EPOCHS, validation_data=val_gen_base, callbacks=[es])\n",
        "vgg_base.save(os.path.join(MODEL_DIR, \"vgg_base_unit2.h5\"))\n",
        "\n",
        "# Evaluate baseline\n",
        "def evaluate_keras_model(model, seq):\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "    for Xb, yb in seq:\n",
        "        probs = model.predict(Xb)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_prob.extend(probs.tolist())\n",
        "        y_true.extend(np.argmax(yb, axis=1).tolist())\n",
        "    return np.array(y_true), np.array(y_pred), np.array(y_prob)\n",
        "\n",
        "y_true_base, y_pred_base, y_prob_base = evaluate_keras_model(vgg_base, test_gen)\n",
        "acc_base = accuracy_score(y_true_base, y_pred_base)\n",
        "f1_base = f1_score(y_true_base, y_pred_base, average='macro')\n",
        "print(\"Baseline: Acc\", acc_base, \"F1\", f1_base)\n",
        "\n",
        "# Train augmented\n",
        "print(\"Treinando VGG16 com dataset aumentado...\")\n",
        "vgg_aug = build_vgg_transfer(num_classes, train_base=False)\n",
        "history_aug = vgg_aug.fit(train_gen_aug, epochs=CLASS_EPOCHS, validation_data=val_gen_aug, callbacks=[es])\n",
        "vgg_aug.save(os.path.join(MODEL_DIR, \"vgg_aug_unit2.h5\"))\n",
        "\n",
        "y_true_aug, y_pred_aug, y_prob_aug = evaluate_keras_model(vgg_aug, test_gen)\n",
        "acc_aug = accuracy_score(y_true_aug, y_pred_aug)\n",
        "f1_aug = f1_score(y_true_aug, y_pred_aug, average='macro')\n",
        "print(\"Augmented: Acc\", acc_aug, \"F1\", f1_aug)"
      ],
      "metadata": {
        "id": "XMUlI35sy_tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 8 - Resultados: tabela, ROC e matrizes de confus√£o\n",
        "# --------------------------\n",
        "print(\"\\nEtapa 8: Gerando m√©tricas, curvas ROC e matrizes de confus√£o (sem PDF).\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Cria√ß√£o da tabela de m√©tricas\n",
        "y_test_bin = label_binarize(y_true_base, classes=np.arange(num_classes))\n",
        "try:\n",
        "    auc_base = roc_auc_score(y_test_bin, y_prob_base, average='macro')\n",
        "except Exception:\n",
        "    auc_base = np.nan\n",
        "try:\n",
        "    auc_aug = roc_auc_score(y_test_bin, y_prob_aug, average='macro')\n",
        "except Exception:\n",
        "    auc_aug = np.nan\n",
        "\n",
        "metrics_df = pd.DataFrame([\n",
        "    [\"VGG16 (baseline)\", acc_base, f1_base, auc_base],\n",
        "    [\"VGG16 (augmented)\", acc_aug, f1_aug, auc_aug]\n",
        "], columns=[\"Model\", \"Accuracy\", \"F1_macro\", \"AUC_macro\"])\n",
        "\n",
        "# Salvar tabela\n",
        "metrics_path = os.path.join(RESULTS_DIR, \"metrics_comparison_unit2.csv\")\n",
        "metrics_df.to_csv(metrics_path, index=False)\n",
        "print(\"\\nüìä Tabela de m√©tricas:\")\n",
        "print(metrics_df)\n",
        "print(f\"\\nTabela salva em: {metrics_path}\")\n",
        "\n",
        "# --------------------------\n",
        "# Curvas ROC\n",
        "# --------------------------\n",
        "def multiclass_roc(y_true, y_prob, n_classes):\n",
        "    y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    for i in range(n_classes):\n",
        "        try:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], np.array(y_prob)[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        except:\n",
        "            fpr[i], tpr[i], roc_auc[i] = None, None, None\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes) if fpr[i] is not None]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        if fpr[i] is not None:\n",
        "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"], tpr[\"macro\"], roc_auc[\"macro\"] = all_fpr, mean_tpr, auc(all_fpr, mean_tpr)\n",
        "    return fpr, tpr, roc_auc\n",
        "\n",
        "fpr_b, tpr_b, roc_auc_b = multiclass_roc(y_true_base, y_prob_base, num_classes)\n",
        "fpr_a, tpr_a, roc_auc_a = multiclass_roc(y_true_aug, y_prob_aug, num_classes)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr_b[\"macro\"], tpr_b[\"macro\"], '--', label=f'Baseline (AUC={roc_auc_b[\"macro\"]:.3f})')\n",
        "plt.plot(fpr_a[\"macro\"], tpr_a[\"macro\"], '-.', label=f'Augmented (AUC={roc_auc_a[\"macro\"]:.3f})')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Compara√ß√£o (Macro Average)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "roc_path = os.path.join(RESULTS_DIR, \"roc_comparison.png\")\n",
        "plt.savefig(roc_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"‚úÖ Curva ROC salva em: {roc_path}\")\n",
        "\n",
        "# --------------------------\n",
        "# Matrizes de confus√£o\n",
        "# --------------------------\n",
        "cm_base = confusion_matrix(y_true_base, y_pred_base)\n",
        "cm_aug  = confusion_matrix(y_true_aug, y_pred_aug)\n",
        "\n",
        "# Baseline\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm_base, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=le_classes.classes_, yticklabels=le_classes.classes_)\n",
        "plt.title(\"Matriz de Confus√£o - CNN Base\")\n",
        "plt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\n",
        "cm_base_path = os.path.join(RESULTS_DIR, \"cm_baseline.png\")\n",
        "plt.savefig(cm_base_path, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# Augmented\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm_aug, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=le_classes.classes_, yticklabels=le_classes.classes_)\n",
        "plt.title(\"Matriz de Confus√£o - CNN com Augmentation\")\n",
        "plt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\n",
        "cm_aug_path = os.path.join(RESULTS_DIR, \"cm_augmented.png\")\n",
        "plt.savefig(cm_aug_path, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(f\"‚úÖ Matrizes salvas em:\\n - {cm_base_path}\\n - {cm_aug_path}\")\n",
        "\n",
        "print(\"\\nüéØ Resultados finais conclu√≠dos! Verifique os arquivos na pasta 'results_unit2'\")\n"
      ],
      "metadata": {
        "id": "SOjhLBn401Pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}